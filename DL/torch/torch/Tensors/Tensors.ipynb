{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c202a11",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/itmorn/AI.handbook/blob/main/DL/torch/torch/Tensors/Tensors.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44658bf0",
   "metadata": {},
   "source": [
    "# 类型判断、全局设置"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d24ca802",
   "metadata": {},
   "source": [
    "## is_tensor\n",
    "Returns True if obj is a PyTorch tensor.  \n",
    "\n",
    "如果obj是一个PyTorch张量，则返回True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb82e851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.tensor([1,2,3])\n",
    "torch.is_tensor(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f89ca697",
   "metadata": {},
   "source": [
    "## is_storage\n",
    "\t\n",
    "Returns True if obj is a PyTorch storage object.  \n",
    "\n",
    "如果obj是PyTorch存储对象，则返回True。\n",
    "\n",
    "在PyTorch中，存储对象（torch.Storage）是一个比张量更底层的数据结构，用于存储和管理数据。与张量不同，存储对象不包含任何形状和维度信息，它们只是简单的一维数组。\n",
    "\n",
    "在实际应用中，存储对象可能不太常用，但它们在某些情况下可以提供性能优势。以下是一些常见的使用场景：\n",
    "\n",
    "1. 在使用自定义的数据类型或格式时，存储对象可以提供一种更灵活的数据存储方式。例如，如果要存储稀疏矩阵，则存储对象可以更有效地存储非零元素。\n",
    "\n",
    "2. 存储对象可以提供一种更高效的内存管理方式，特别是在处理大型数据集时。例如，在处理大型图像或音频数据时，使用存储对象可以更有效地管理内存和数据加载。\n",
    "\n",
    "3. 存储对象可以用于实现自定义张量操作，例如，可以将存储对象用于实现卷积和池化操作中的内部存储。\n",
    "\n",
    "总之，使用存储对象需要对PyTorch底层的内存管理和数据操作有一定的了解，但它们可以在某些情况下提供性能优势。通常情况下，如果不需要直接访问存储对象，则建议使用张量来管理数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ce252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "storage = torch.FloatStorage(10)\n",
    "print(torch.is_storage(storage))\n",
    "\n",
    "\n",
    "x=torch.tensor([1,2,3])\n",
    "print(torch.is_storage(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7a46670",
   "metadata": {},
   "source": [
    "## is_complex\n",
    "\t\n",
    "Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.  \n",
    "\n",
    "如果输入数据的数据类型是复数类型，即torch.complex64或torch.complex128，则返回True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c9cffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1+2j, 3-4j], dtype=torch.complex128)\n",
    "\n",
    "print(torch.is_complex(x))  # True\n",
    "# 在这个例子中，我们创建了一个数据类型为torch.complex128的复数张量x，\n",
    "# 然后使用torch.is_complex()函数检查x的数据类型是否为复数类型，结果返回True。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1427cbf8",
   "metadata": {},
   "source": [
    "## is_conj\n",
    "\n",
    "Returns True if the input is a conjugated tensor, i.e. its conjugate bit is set to True.\n",
    "\n",
    "如果输入的张量是共轭张量，即其共轭标志位(conjugate bit)设置为True，则返回True。\n",
    "\n",
    "在数学中，一个复数的共轭是将其虚部变号，实部不变的一种操作。类似地，对于一个复数张量，将其每个元素共轭得到的新张量称为共轭张量(conjugate tensor)。\n",
    "\n",
    "在PyTorch中，使用torch.conj()函数可以求得一个张量的共轭张量。共轭张量在信号处理、光学等领域中有广泛的应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08dc70d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.-2.j, 3.+4.j], dtype=torch.complex128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1+2j, 3-4j], dtype=torch.complex128)\n",
    "y = torch.conj(x)\n",
    "\n",
    "print(torch.is_conj(y))  # True\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f27322e0",
   "metadata": {},
   "source": [
    "## is_floating_point\n",
    "\t\n",
    "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.\n",
    "\n",
    "如果输入数据的数据类型是浮点数类型，即torch.float64、torch.float32、torch.float16或torch.bfloat16之一，则返回True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28910691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([1, 2, 3])\n",
    "\n",
    "print(torch.is_floating_point(x))  # True\n",
    "print(torch.is_floating_point(y))  # False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4baa9990",
   "metadata": {},
   "source": [
    "## is_nonzero\n",
    "\t\n",
    "Returns True if the input is a single element tensor which is not equal to zero after type conversions.\n",
    "\n",
    "如果输入是一个单元素张量，并且在类型转换后不等于零，即不等于torch.tensor([0.])、torch.tensor([0])或torch.tensor([False])，则返回True。如果torch.numel()不等于1（即张量元素个数不为1），则抛出一个运行时错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b25cd10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([3], dtype=torch.float32)\n",
    "y = torch.tensor([0])\n",
    "z = torch.tensor([True])\n",
    "\n",
    "print(torch.is_nonzero(x))  # True\n",
    "print(torch.is_nonzero(y))  # False\n",
    "print(torch.is_nonzero(z))  # True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c3de1b8",
   "metadata": {},
   "source": [
    "## set_default_dtype\n",
    "\t\n",
    "Sets the default floating point dtype to d.\n",
    "\n",
    "将默认的浮点数数据类型设置为d。支持输入torch.float32和torch.float64。其他数据类型可能会被接受但不会报错，但不受支持，可能不会按预期工作。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc2d1549",
   "metadata": {},
   "source": [
    "## get_default_dtype\n",
    "\n",
    "Get the current default floating point torch.dtype.\n",
    "\n",
    "获取默认浮点数数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a9436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.complex64\n",
      "==================================================\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.complex128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_default_dtype(torch.float32) #默认\n",
    "print(torch.get_default_dtype())  # torch.float32\n",
    "print(torch.tensor([1.2, 3]).dtype)\n",
    "print(torch.tensor([1.2, 3j]).dtype)\n",
    "print(\"=\"*50)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "print(torch.get_default_dtype())\n",
    "print(torch.tensor([1.2, 3]).dtype)\n",
    "print(torch.tensor([1.2, 3j]).dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d4620bc",
   "metadata": {},
   "source": [
    "## set_default_tensor_type\n",
    "\n",
    "Sets the default torch.Tensor type to floating point tensor type t.\n",
    "\n",
    "将默认的torch.Tensor类型设置为浮点数张量类型t。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d33739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "print(torch.tensor([1.2, 3]).dtype)\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(torch.tensor([1.2, 3]).dtype)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1619fc6",
   "metadata": {},
   "source": [
    "## set_default_tensor_type和set_default_dtype的适用场景\n",
    "set_default_tensor_type()\n",
    "- 当你需要在不同设备上运行代码时，可以使用set_default_tensor_type()设置默认的张量类型。例如，如果你需要在GPU上训练模型，可以将默认的张量类型设置为torch.cuda.FloatTensor，这样在创建新的张量时将默认使用这种类型。\n",
    "\n",
    "- 当你需要在创建大量的张量时，可以使用set_default_tensor_type()设置默认的张量类型。例如，在生成大量数据集时，如果你需要使用GPU加速，可以将默认的张量类型设置为torch.cuda.FloatTensor。\n",
    "\n",
    "\n",
    "set_default_dtype()\n",
    "- 当你需要在执行数学运算时使用特定的数据类型时，可以使用set_default_dtype()设置默认的数据类型。例如，在数值计算中，float32通常足够精确，但在某些情况下，你可能需要更高的精度，这时你可以将默认的数据类型设置为float64。\n",
    "\n",
    "- 当你需要在创建张量时，使用与上下文匹配的默认数据类型时，可以使用set_default_dtype()设置默认的数据类型。例如，当你需要在不同计算机上运行相同的代码时，可以使用这个函数来确保数据类型在各种环境中的一致性。\n",
    "\n",
    "需要注意的是，这两个函数都是全局设置，会影响到整个程序，因此在使用它们时需要谨慎。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f12ea84",
   "metadata": {},
   "source": [
    "## numel\n",
    "Returns the total number of elements in the input tensor.\n",
    "返回输入张量中元素的总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76964f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2, 3)\n",
    "num_elements = torch.numel(x)\n",
    "print(num_elements)  # 输出 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b556ef02",
   "metadata": {},
   "source": [
    "## set_printoptions\n",
    "Set options for printing.\n",
    "\n",
    "torch.set_printoptions函数可以用于设置张量打印的选项，包括精度、格式等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ed5a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.12])\n",
      "tensor([0, 1, 2,  ..., 7, 8, 9])\n",
      "tensor([1.1235])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Limit the precision of elements\n",
    "torch.set_printoptions(precision=2)\n",
    "print(torch.tensor([1.12345]))\n",
    "\n",
    "# Limit the number of elements shown\n",
    "torch.set_printoptions(threshold=5)\n",
    "print(torch.arange(10))\n",
    "\n",
    "# Restore defaults\n",
    "torch.set_printoptions(profile='default')\n",
    "print(torch.tensor([1.12345]))\n",
    "print(torch.arange(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "316e2812",
   "metadata": {},
   "source": [
    "## set_flush_denormal\n",
    "Disables denormal floating numbers on CPU.\n",
    "\n",
    "禁用CPU上的非规格化浮点数。\n",
    "\n",
    "在计算机科学中，非规格化浮点数是一种特殊的浮点数形式，其指数位为全0而尾数位非0。这种数通常表示接近于0的数值，但具有较低的精度和性能代价。禁用非规格化浮点数可以提高计算精度和效率。\n",
    "\n",
    "\n",
    " 当在CPU上执行大量的浮点计算时，将浮点数规范化为正常值可能会降低性能。 因此，禁用浮点数规范化可能会提高性能。 但是，请注意，禁用规范化可能会影响计算的精度，并且可能会导致不一致的行为。 因此，应该在使用之前进行仔细的测试和评估，确保其对应用程序的性能和精度产生积极的影响。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bbf8606",
   "metadata": {},
   "source": [
    "# 创建操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28567c7a",
   "metadata": {},
   "source": [
    "## tensor\n",
    "在PyTorch中，当我们使用torch.Tensor()或者torch.tensor()函数创建一个新的张量时，这个张量就会被标记为“无autograd history”，也就是说，它没有任何与自动求导相关的历史记录。这意味着我们不能对这个张量进行自动求导。\n",
    "\n",
    "\n",
    "**定义**：\n",
    "torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- data (array_like) – Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types.  张量的初始数据。可以是列表、元组、NumPy ndarray、标量和其他类型。\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, infers data type from data.   返回张量的期望数据类型。默认值:如果为None，则从数据推断数据类型。\n",
    "\n",
    "- device (torch.device, optional) – the device of the constructed tensor. If None and data is a tensor then the device of data is used. If None and data is not a tensor then the result tensor is constructed on the CPU.  构造张量的设备。如果None且data是一个张量，则使用data的设备。如果None且data不是一个张量，那么结果张量将在CPU上构造。\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.  是否记录张量的梯度\n",
    "\n",
    "- pin_memory (bool, optional) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False.  如果设置，返回的张量将分配到固定内存中。\n",
    "pin_memory 是 PyTorch 中一种优化技术，它可以在 CPU 和 GPU 之间高速地复制数据，以加速数据的传输。在 PyTorch 中，可以通过在 Dataset 类中设置 pin_memory=True 来启用 pin_memory 技术。需要注意的是，pin_memory 技术需要更多的内存来存储固定内存区域中的数据，因此如果内存有限，可能需要考虑关闭 pin_memory 技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae59a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 1.2000],\n",
      "        [2.2000, 3.1000],\n",
      "        [4.9000, 5.2000]])\n",
      "tensor([0, 1])\n",
      "tensor([0.5000, 1.0000], requires_grad=True)\n",
      "tensor([[0.1111, 0.2222, 0.3333]], dtype=torch.float64)\n",
      "tensor(3.1416)\n",
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]]))\n",
    "\n",
    "print(torch.tensor([0, 1]))  # Type inference on data\n",
    "\n",
    "print(torch.tensor([0.5, 1],requires_grad=True))  # Type inference on data\n",
    "\n",
    "print(torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
    "             dtype=torch.float64,\n",
    "             device=torch.device('cpu')) )\n",
    "            #  device=torch.device('cuda:0'))  # creates a double tensor on a CUDA device\n",
    "\n",
    "print(torch.tensor(3.14159))  # Create a zero-dimensional (scalar) tensor\n",
    "\n",
    "print(torch.tensor([]))  # Create an empty tensor (of size (0,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55885736",
   "metadata": {},
   "source": [
    "## sparse_coo_tensor\n",
    "sparse_coo_tensor 是一种用于创建稀疏张量的函数。稀疏张量是一种特殊的张量，其中只有一小部分元素是非零值，大部分元素都是零值。与密集张量相比，稀疏张量可以更高效地存储和计算，因为它们只需要存储非零值和对应的索引。\n",
    "\n",
    " **定义**：\n",
    " torch.sparse_coo_tensor(indices, values, size=None, *, dtype=None, device=None, requires_grad=False) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- indices (array_like) – Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor internally. The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.  一个二维的 LongTensor，表示非零元素的索引。\n",
    "\n",
    "- values (array_like) – Initial values for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types.    一个一维的 Tensor，表示非零元素的值。\n",
    "\n",
    "- size (list, tuple, or torch.Size, optional) – Size of the sparse tensor. If not provided the size will be inferred as the minimum size big enough to hold all non-zero elements.  一个元组，表示稀疏张量的形状。\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, infers data type from values.   返回张量的期望数据类型。默认值:如果为None，则从值推断数据类型。\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.  返回张量的期望设备\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.   是否记录张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c96c38bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 3., 0.],\n",
      "        [4., 0., 5., 0.]])\n",
      "tensor([[0., 0., 3.],\n",
      "        [4., 0., 5.]])\n",
      "tensor([[0., 0., 3., 0.],\n",
      "        [4., 0., 5., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "i = torch.tensor([[0, 1, 1],\n",
    "                  [2, 0, 2]])\n",
    "v = torch.tensor([3, 4, 5], dtype=torch.float32)\n",
    "sparse_tensor = torch.sparse_coo_tensor(i, v, [2, 4])\n",
    "print(sparse_tensor.to_dense())\n",
    "\n",
    "sparse_tensor = torch.sparse_coo_tensor(i, v)  # Shape inference\n",
    "print(sparse_tensor.to_dense())\n",
    "\n",
    "sparse_tensor = torch.sparse_coo_tensor(i, v, [2, 4],\n",
    "                        dtype=torch.float64)\n",
    "print(sparse_tensor.to_dense())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8077552",
   "metadata": {},
   "source": [
    "## asarray\n",
    "将obj转换为一个张量。\n",
    "\n",
    " **定义**：\n",
    "torch.asarray(obj, *, dtype=None, device=None, copy=None, requires_grad=False) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- obj (object) – a tensor, NumPy array, DLPack Capsule, object that implements Python’s buffer protocol, scalar, or sequence of scalars.  一个张量，NumPy数组，DLPack胶囊，实现Python的缓冲区协议，标量或标量序列的对象。\n",
    "\n",
    "- dtype (torch.dtype, optional) – the datatype of the returned tensor. Default: None, which causes the datatype of the returned tensor to be inferred from obj.  返回张量的数据类型\n",
    "\n",
    "- copy (bool, optional) – controls whether the returned tensor shares memory with obj. Default: None, which causes the returned tensor to share memory with obj whenever possible. If True then the returned tensor does not share its memory. If False then the returned tensor shares its memory with obj and an error is thrown if it cannot.    控制返回的张量是否与obj共享内存。默认值:None，这使得返回的张量尽可能地与obj共享内存.\n",
    "\n",
    "- device (torch.device, optional) – the device of the returned tensor. Default: None, which causes the device of obj to be used.  返回张量的设备\n",
    "\n",
    "- requires_grad (bool, optional) – whether the returned tensor requires grad. Default: False, which causes the returned tensor not to require a gradient. If True, then the returned tensor will require a gradient, and if obj is also a tensor with an autograd history then the returned tensor will have the same history.   是否记录张量的梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67c20fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1, 2, 3])\n",
    "# Shares memory with tensor 'a'\n",
    "b = torch.asarray(a)\n",
    "print(a.data_ptr() == b.data_ptr())\n",
    "\n",
    "# Forces memory copy\n",
    "c = torch.asarray(a, copy=True)\n",
    "print(a.data_ptr() == c.data_ptr()) # 数据指针"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cd7048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4., 5.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3., 4., 5.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1., 2, 3], requires_grad=True)\n",
    "b = a + 2\n",
    "\n",
    "# Shares memory with tensor 'b', with no grad\n",
    "c = torch.asarray(b)\n",
    "print(c)\n",
    "\n",
    "# Shares memory with tensor 'b', retaining autograd history\n",
    "d = torch.asarray(b, requires_grad=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16178396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "array = numpy.array([1, 2, 3])\n",
    "# Shares memory with array 'array'\n",
    "t1 = torch.asarray(array)\n",
    "print(array.__array_interface__['data'][0] == t1.data_ptr())\n",
    "\n",
    "# Copies memory due to dtype mismatch\n",
    "t2 = torch.asarray(array, dtype=torch.float32)\n",
    "print(array.__array_interface__['data'][0] == t2.data_ptr())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "480c28ab",
   "metadata": {},
   "source": [
    "## as_tensor\n",
    "将数据转换为一个张量，如果可能的话，共享数据并保存自动梯度历史。\n",
    "\n",
    "如果数据已经是具有所请求的dtype和device的张量，则返回数据本身，但如果数据是具有不同dtype或device的张量，则复制它，就像使用数据一样。\n",
    "\n",
    "\n",
    "对于 NumPy 数组，确实可以使用 copy=False 的方式来控制是否共享内存。但是对于其他类型的输入数据（如 Python 列表、Python 元组等），copy=False 并不能控制内存共享。此外，即使是对于 NumPy 数组，使用 copy=False 也并不总是能够共享内存，这取决于输入数组是否满足共享内存的条件。\n",
    "\n",
    "相比之下，torch.as_tensor() 的共享内存特性更加稳定和可靠，它可以确保在所有情况下都尽可能地共享内存，而不需要依赖于输入数据的类型和属性。另外，torch.as_tensor() 还提供了更精细的控制，例如可以指定数据类型、设备等参数，以满足不同的需求。\n",
    "\n",
    "因此，在需要共享内存和更精细控制输出张量属性的情况下，推荐使用 torch.as_tensor()。而如果只是简单地将 NumPy 数组或标量转换为 PyTorch 张量，而不需要特别控制输出张量的属性，那么 torch.asarray() 也是一个不错的选择。\n",
    "\n",
    " **定义**：\n",
    " torch.as_tensor(data, dtype=None, device=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- data (array_like) – Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types.  张量的初始数据。可以是列表、元组、NumPy ndarray、标量和其他类型。\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, infers data type from data.  返回张量的期望数据类型。默认值:如果为None，则从数据推断数据类型。\n",
    "\n",
    "- device (torch.device, optional) – the device of the constructed tensor. If None and data is a tensor then the device of data is used. If None and data is not a tensor then the result tensor is constructed on the CPU.  构造张量的设备。如果None且data是一个张量，则使用data的设备。如果None且data不是一个张量，那么结果张量将在CPU上构造。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8cc1ce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  2  3]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "a = numpy.array([1, 2, 3])\n",
    "t = torch.as_tensor(a)\n",
    "t[0] = -1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f81b06c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "a = numpy.array([1, 2, 3])\n",
    "t = torch.as_tensor(a, dtype=torch.float64)\n",
    "# t = torch.as_tensor(a, device=torch.device('cuda'))\n",
    "t[0] = -1\n",
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fd61c95",
   "metadata": {},
   "source": [
    "## as_strided\n",
    "torch.as_strided是一个 PyTorch 的张量操作函数，它可以基于现有的张量创建一个新的张量，该新张量具有指定大小和步幅的视图。\n",
    "\n",
    "具体来说，该函数会将输入张量 input 视为一个一维的存储（即将多维张量展平为一维），并根据指定的大小 size 和步幅 stride 重新构造一个新的张量。新张量的形状和大小由 size 参数决定，步幅由 stride 参数决定。具体地，新张量在第 i 维的步幅为 stride[i]，大小为 size[i]。\n",
    "\n",
    "storage_offset 参数表示新张量在存储中的偏移量，可以用于构造多个视图共享相同的存储空间。如果不指定 storage_offset 参数，则默认为 0。\n",
    "\n",
    "需要注意的是，使用 torch.as_strided() 构造的新张量仅仅是输入张量的视图，它与输入张量共享存储空间。如果输入张量的数据发生改变，那么新张量的数据也会相应地改变。\n",
    "\n",
    "使用 torch.as_strided() 函数需要非常小心，因为它可以创建一些不符合规范的张量，例如张量的视图可能会超出存储的边界，或者在处理时可能会出现内存访问错误等问题。因此，通常情况下不建议直接使用该函数，而是建议使用更安全的张量操作函数来实现相应的功能。\n",
    "\n",
    " **定义**：\n",
    "torch.as_strided(input, size, stride, storage_offset=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.  输入张量\n",
    "\n",
    "- size (tuple or ints) – the shape of the output tensor  输出张量的形状\n",
    "\n",
    "- stride (tuple or ints) – the stride of the output tensor  输出张量的步幅\n",
    "\n",
    "- storage_offset (int, optional) – the offset in the underlying storage of the output tensor.  输出张量在底层存储中的偏移量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f872f737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77, 78, 79, 80])\n",
      "tensor([[ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18],\n",
      "        [ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19],\n",
      "        [ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20],\n",
      "        [ 3,  5,  7,  9, 11, 13, 15, 17, 19, 21],\n",
      "        [ 4,  6,  8, 10, 12, 14, 16, 18, 20, 22],\n",
      "        [ 5,  7,  9, 11, 13, 15, 17, 19, 21, 23],\n",
      "        [ 6,  8, 10, 12, 14, 16, 18, 20, 22, 24],\n",
      "        [ 7,  9, 11, 13, 15, 17, 19, 21, 23, 25],\n",
      "        [ 8, 10, 12, 14, 16, 18, 20, 22, 24, 26],\n",
      "        [ 9, 11, 13, 15, 17, 19, 21, 23, 25, 27]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19],\n",
       "        [ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20],\n",
       "        [ 3,  5,  7,  9, 11, 13, 15, 17, 19, 21],\n",
       "        [ 4,  6,  8, 10, 12, 14, 16, 18, 20, 22],\n",
       "        [ 5,  7,  9, 11, 13, 15, 17, 19, 21, 23],\n",
       "        [ 6,  8, 10, 12, 14, 16, 18, 20, 22, 24],\n",
       "        [ 7,  9, 11, 13, 15, 17, 19, 21, 23, 25],\n",
       "        [ 8, 10, 12, 14, 16, 18, 20, 22, 24, 26],\n",
       "        [ 9, 11, 13, 15, 17, 19, 21, 23, 25, 27],\n",
       "        [10, 12, 14, 16, 18, 20, 22, 24, 26, 28]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(81).reshape(9,9) # 不加reshape也是一样的结果\n",
    "print(x)\n",
    "t = torch.as_strided(x, size=(10, 10), stride=(1, 2))\n",
    "print(t)\n",
    "t = torch.as_strided(x, size=(10, 10), stride=(1, 2), storage_offset=1)\n",
    "t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "173fffd6",
   "metadata": {},
   "source": [
    "## from_numpy\n",
    "从numpy.ndarray创建一个张量。\n",
    "\n",
    " **定义**：\n",
    "torch.from_numpy(ndarray) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5ff3aaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  2,  3])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = numpy.array([1, 2, 3])\n",
    "t = torch.from_numpy(a)\n",
    "t\n",
    "t[0] = -1\n",
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ac281db",
   "metadata": {},
   "source": [
    "## from_dlpack\n",
    "将一个张量从外部库转换为torch.Tensor。\n",
    "\n",
    "DLPack是一个轻量级的跨框架张量内存交换标准，它定义了一种描述张量数据内存布局的结构体。通过DLPack，可以将一个深度学习框架的张量数据转换为DLPack tensor并传递给另一个框架处理，从而实现模型组合和迁移等功能。DLPack已经被多个深度学习框架所支持。\n",
    "\n",
    " **定义**：\n",
    "torch.from_dlpack(ext_tensor) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f045d1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -1,  2,  3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.dlpack\n",
    "t = torch.arange(4)\n",
    "\n",
    "t2 = torch.from_dlpack(t)\n",
    "t2[:2] = -1  # show that memory is shared\n",
    "t2\n",
    "t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58449e35",
   "metadata": {},
   "source": [
    "## frombuffer\n",
    "从实现Python缓冲区协议的对象创建一个一维张量。\n",
    "\n",
    " **定义**：\n",
    "torch.frombuffer(buffer, *, dtype, count=- 1, offset=0, requires_grad=False) → Tensor\n",
    "\n",
    "**参数**：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1c04b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array('i', [-1, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([255], dtype=torch.int32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import array\n",
    "import torch\n",
    "a = array.array('i', [1, 2, 3])\n",
    "t = torch.frombuffer(a, dtype=torch.int32)\n",
    "t\n",
    "t[0] = -1\n",
    "print(a)\n",
    "\n",
    "# Interprets the signed char bytes as 32-bit integers.\n",
    "# Each 4 signed char elements will be interpreted as\n",
    "# 1 signed 32-bit integer.\n",
    "a = array.array('b', [-1, 0, 0, 0])\n",
    "torch.frombuffer(a, dtype=torch.int32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0546fec",
   "metadata": {},
   "source": [
    "## zeros\n",
    "返回一个用标量值0填充的张量，其形状由变量参数size定义。\n",
    "\n",
    " **定义**：\n",
    "torch.zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()).  返回张量的期望数据类型\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided.  返回张量的期望布局,有 strided， sparse_coo(稀疏张量使用)等\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.  构造张量的设备\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False. 是否记录张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "360ffb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(2, 3)\n",
    "b = torch.zeros(2,3,out=a)\n",
    "print(a.data_ptr() == b.data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6923ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor(indices=tensor([], size=(2, 0)),\n",
      "       values=tensor([], size=(0,)),\n",
      "       size=(2, 3), nnz=0, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个大小为 (2, 3) 的张量，存储顺序为行优先\n",
    "x = torch.zeros((2, 3), layout=torch.strided)\n",
    "print(x)\n",
    "\n",
    "# 创建一个大小为 (2, 3) 的张量，存储顺序为列优先\n",
    "y = torch.zeros((2, 3), layout=torch.sparse_coo)\n",
    "print(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b69a3bc",
   "metadata": {},
   "source": [
    "## zeros_like\n",
    "返回一个用标量值0填充的张量，大小与输入相同。\n",
    "\n",
    " **定义**：\n",
    "torch.zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the size of input will determine size of the output tensor.  输入的大小将决定输出张量的大小。\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned Tensor. Default: if None, defaults to the dtype of input.  返回张量的期望数据类型。Default:如果为None，默认为输入的dtype。\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned tensor. Default: if None, defaults to the layout of input.  返回张量的期望布局。Default:如果为None，默认为输入的布局。\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, defaults to the device of input.  返回张量的期望设备。Default:如果为None，则默认为输入设备。\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.  是否记录张量的梯度\n",
    "\n",
    "- memory_format (torch.memory_format, optional) – the desired memory format of returned Tensor. Default: torch.preserve_format.  返回的张量的期望内存格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "19874059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input = torch.empty(2, 3)\n",
    "torch.zeros_like(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41f6d35d",
   "metadata": {},
   "source": [
    "## ones\n",
    "\n",
    " **定义**：\n",
    "torch.ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- size (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.   一个定义输出张量形状的整数序列。可以是可变数量的参数，也可以是列表或元组之类的集合。\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()).  返回张量的期望数据类型。\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided.  返回张量的期望布局。Default:如果为None，默认为输入的布局。\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.  返回张量的期望设备。Default:如果为None，则默认为输入设备。\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.  是否记录张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ebe20281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones(2, 3)\n",
    "\n",
    "torch.ones(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17e1aebb",
   "metadata": {},
   "source": [
    "## ones_like\n",
    "返回一个用标量值1填充的张量，大小与输入相同。\n",
    "\n",
    " **定义**：\n",
    "torch.ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the size of input will determine size of the output tensor.  输入的大小将决定输出张量的大小。\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned Tensor. Default: if None, defaults to the dtype of input.  返回张量的期望数据类型。Default:如果为None，默认为输入的dtype。\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned tensor. Default: if None, defaults to the layout of input.  返回张量的期望布局。Default:如果为None，默认为输入的布局。\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, defaults to the device of input.  返回张量的期望设备。Default:如果为None，则默认为输入设备。\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.  是否记录张量的梯度\n",
    "\n",
    "- memory_format (torch.memory_format, optional) – the desired memory format of returned Tensor. Default: torch.preserve_format.  返回的张量的期望内存格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6dde5c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.empty(2, 3)\n",
    "torch.ones_like(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89a25106",
   "metadata": {},
   "source": [
    "## arange\n",
    "\n",
    " **定义**：\n",
    "torch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- start (Number) – the starting value for the set of points. Default: 0.  点集的起始值。默认值:0。\n",
    "\n",
    "- end (Number) – the ending value for the set of points  点集的结束值\n",
    "\n",
    "- step (Number) – the gap between each pair of adjacent points. Default: 1.  每对相邻点之间的距离。默认值:1。\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input arguments. If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see get_default_dtype(). Otherwise, the dtype is inferred to be torch.int64.  返回张量的期望数据类型。Default:如果为None，则使用全局默认值\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided.  返回张量的期望布局。Default:如果为None，默认为输入的布局。\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.  返回张量的期望设备。Default:如果为None，则默认为输入设备。\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.  是否记录张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bcc8c9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.5000, 2.0000])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.arange(5)\n",
    "torch.arange(1, 4)\n",
    "torch.arange(1, 2.5, 0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b223a7d9",
   "metadata": {},
   "source": [
    "## linspace\n",
    "创建一个一维大小步长的张量，其值从开始到结束都是均匀间隔的.\n",
    "\n",
    " **定义**：\n",
    "torch.linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- start (float) – the starting value for the set of points  点集的起始值\n",
    "\n",
    "- end (float) – the ending value for the set of points  点集的结束值\n",
    "\n",
    "- steps (int) – size of the constructed tensor  构造张量的大小\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存\n",
    "\n",
    "- dtype (torch.dtype, optional) – the data type to perform the computation in. Default: if None, uses the global default dtype (see torch.get_default_dtype()) when both start and end are real, and corresponding complex dtype when either is complex.  返回张量的期望数据类型。Default:如果为None，则使用全局默认值\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided.  返回张量的期望布局。Default:如果为None，默认为输入的布局。\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.  返回张量的期望设备\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.  是否记录张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9831c4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.0000,  -7.7778,  -5.5556,  -3.3333,  -1.1111,   1.1111,   3.3333,\n",
       "          5.5556,   7.7778,  10.0000])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.linspace(3, 10, steps=5)\n",
    "torch.linspace(-10, 10, steps=5)\n",
    "torch.linspace(start=-10, end=10, steps=5)\n",
    "torch.linspace(start=-10, end=10, steps=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0c2e0ce",
   "metadata": {},
   "source": [
    "## logspace\n",
    "生成在对数空间中均匀分布的数值.\n",
    "$$(\\text{base}^{\\text{start}},\\text{base}^{\\left(\\text{stand}+\\frac{\\text{end}-\\text{start}}{\\text{steps}-1}\\right)},\\ldots,\\text{base}^{\\left(\\text{start}+\\left(\\text{steps}-2\\right)*\\frac{\\text{end}-\\text{start}}{{\\text{steps}-1}}\\right)},\\text{base}^\\text{end})$$\n",
    "\n",
    " **定义**：\n",
    "torch.logspace(start, end, steps, base=10.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- start (float) – the starting value for the set of points 点集的起始值\n",
    "\n",
    "- end (float) – the ending value for the set of points  点集的结束值\n",
    "\n",
    "- steps (int) – size of the constructed tensor  构造张量的大小\n",
    "\n",
    "- base (float, optional) – base of the logarithm function. Default: 10.0.  对数函数的基数。默认值:10.0。\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存\n",
    "\n",
    "- dtype (torch.dtype, optional) – the data type to perform the computation in. Default: if None, uses the global default dtype (see torch.get_default_dtype()) when both start and end are real, and corresponding complex dtype when either is complex.  执行计算的数据类型\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided.  返回张量的期望布局。Default:如果为None，默认为输入的布局。\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.  返回张量的期望设备。Default:如果为None，则默认为输入设备。\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.  是否记录张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "05a49486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2.,    4.,    8.,   16.,   32.,   64.,  128.,  256.,  512., 1024.])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.logspace(start=-10, end=10, steps=5)\n",
    "torch.logspace(start=0.1, end=1.0, steps=5)\n",
    "torch.logspace(start=0.1, end=1.0, steps=1)\n",
    "torch.logspace(start=2, end=2, steps=1, base=2)\n",
    "torch.logspace(start=1, end=10, steps=10, base=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f2a5a93",
   "metadata": {},
   "source": [
    "## eye\n",
    "返回一个二维张量，对角线上为1，其他地方为0。\n",
    "\n",
    " **定义**：\n",
    "torch.eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- n (int) – the number of rows  行数\n",
    "\n",
    "- m (int, optional) – the number of columns with default being n  默认为n的列数\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()).  执行计算的数据类型\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided.  返回张量的期望布局。Default:如果为None，默认为输入的布局。\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.  Default:如果为None，则默认为输入设备。\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.  是否记录张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "484bb611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.eye(3,4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48e0999b",
   "metadata": {},
   "source": [
    "## empty\n",
    "返回一个充满未初始化数据的张量。张量的形状由变量参数大小定义。\n",
    "\n",
    " **定义**：\n",
    "torch.empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- size (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.  一个定义输出张量形状的整数序列。\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()).  返回张量的期望数据类型\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided.  返回张量的期望布局。Default:如果为None，默认为输入的布局。\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.  返回张量的期望设备。Default:如果为None，则使用当前设备作为默认张量类型\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.  是否记录张量的梯度\n",
    "\n",
    "- pin_memory (bool, optional) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False.如果设置，返回的张量将分配到固定内存中。pin_memory 是 PyTorch 中一种优化技术，它可以在 CPU 和 GPU 之间高速地复制数据，以加速数据的传输。在 PyTorch 中，可以通过在 Dataset 类中设置 pin_memory=True 来启用 pin_memory 技术。需要注意的是，pin_memory 技术需要更多的内存来存储固定内存区域中的数据，因此如果内存有限，可能需要考虑关闭 pin_memory 技术。\n",
    "\n",
    "- memory_format (torch.memory_format, optional) – the desired memory format of returned Tensor. Default: torch.contiguous_format.  返回的张量的期望内存格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f7ddf021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 7.1466e-44, 1.8754e+28],\n",
       "        [1.0396e-05, 2.1180e+23, 1.0425e-11]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.empty((2,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31f35f74",
   "metadata": {},
   "source": [
    "## empty_like\n",
    "返回一个与输入大小相同的未初始化张量。\n",
    "\n",
    " **定义**：\n",
    "torch.empty_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the size of input will determine size of the output tensor.  输入的大小将决定输出张量的大小。\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. 返回张量的期望数据类型\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned tensor. Default: if None, defaults to the layout of input. 返回张量的期望布局。Default:如果为None，默认为输入的布局。\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, defaults to the device of input.  返回张量的期望设备。Default:如果为None，则使用当前设备作为默认张量类型\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.  是否记录张量的梯度\n",
    "\n",
    "- memory_format (torch.memory_format, optional) – the desired memory format of returned Tensor. Default: torch.preserve_format.  返回的张量的期望内存格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c1c752a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 842607925, 1663906402, 1650745655],\n",
       "        [ 878798128,  775238498, 1667594341]], dtype=torch.int32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.empty((2,3), dtype=torch.int32, device = 'cpu')\n",
    "torch.empty_like(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28197b48",
   "metadata": {},
   "source": [
    "## full\n",
    "\n",
    "\n",
    " **定义**：\n",
    "torch.full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- size (int...) – a list, tuple, or torch.Size of integers defining the shape of the output tensor.  列表、元组或火炬。定义输出张量形状的整数的大小。\n",
    "\n",
    "- fill_value (Scalar) – the value to fill the output tensor with.  用来填充输出张量的值。\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()).  返回张量的期望数据类型。\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned Tensor. Default: torch.strided.   返回张量的期望布局。Default:如果为None，默认为输入的布局。\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.  返回张量的期望设备\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.  是否记录张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "efc90f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1416, 3.1416, 3.1416],\n",
       "        [3.1416, 3.1416, 3.1416]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.full((2, 3), 3.141592)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ee72107",
   "metadata": {},
   "source": [
    "## full_like\n",
    "返回一个与输入相同大小的张量，填充fill_value\n",
    "\n",
    " **定义**：\n",
    "torch.full_like(input, fill_value, *, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the size of input will determine size of the output tensor.  输入的大小将决定输出张量的大小。\n",
    "\n",
    "- fill_value – the number to fill the output tensor with.  填充输出张量的数字。\n",
    "\n",
    "- dtype (torch.dtype, optional) – the desired data type of returned Tensor. Default: if None, defaults to the dtype of input.  返回张量的期望数据类型\n",
    "\n",
    "- layout (torch.layout, optional) – the desired layout of returned tensor. Default: if None, defaults to the layout of input.  返回张量的期望布局\n",
    "\n",
    "- device (torch.device, optional) – the desired device of returned tensor. Default: if None, defaults to the device of input.  返回张量的期望设备\n",
    "\n",
    "- requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: False.是否记录张量的梯度\n",
    "\n",
    "- memory_format (torch.memory_format, optional) – the desired memory format of returned Tensor. Default: torch.preserve_format.  返回的张量的期望内存格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9c8c4374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 2],\n",
       "        [2, 2, 2]], dtype=torch.int32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.empty((2,3), dtype=torch.int32, device = 'cpu')\n",
    "torch.full_like(a,fill_value=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c91bc5f",
   "metadata": {},
   "source": [
    "## quantize_per_tensor\n",
    "Converts a float tensor to a quantized tensor with given scale and zero point.\n",
    "\n",
    "用于将一个 float 类型的张量量化为一个定点数张量\n",
    "\n",
    "在深度学习中，通常会使用浮点数来表示神经网络中的参数和计算结果。然而，在一些硬件设备（如边缘设备、嵌入式设备）中，浮点数计算的复杂度和存储需求都很高，因此无法直接使用浮点数进行计算。为了在这些设备上运行深度学习模型，需要将浮点数转换为定点数。\n",
    "\n",
    "定点数是一种用整数表示的数值类型，其表示范围和精度可以通过量化比例和量化零点来控制。量化比例用于控制数值范围，量化零点用于控制数值精度。\n",
    "\n",
    "量化的过程就是将浮点数转换为定点数的过程。在 PyTorch 中，使用 quantize_per_tensor 函数可以将一个浮点数张量量化为一个定点数张量。具体来说，quantize_per_tensor 函数会将浮点数除以一个量化比例 scale 并四舍五入取整，然后加上一个量化零点 zero_point 得到一个整数。量化后的整数值将使用指定的数据类型来存储，其范围由量化比例和量化零点决定。\n",
    "\n",
    "例如，假设我们要将一个浮点数 0.1 量化为一个 uint8 类型的定点数。我们可以指定量化比例为 0.01，量化零点为 128。量化后的整数值为：  \n",
    "quantized_value = round(float_value / scale) + zero_point = round(0.1 / 0.01) + 128 = 138\n",
    "因此，浮点数 0.1 将被量化为整数 138。同样，我们可以将其他浮点数量化为定点数。\n",
    "\n",
    "在使用定点数进行计算时，需要进行一定的数值范围和精度的调整。具体来说，定点数的数值范围和精度可能与浮点数不同，因此需要进行量化后的数值范围和精度的调整，以保证计算的正确性。\n",
    "\n",
    "计算和反量化的先后顺序如下：\n",
    "\n",
    "1. 计算前，将浮点数张量量化为定点数张量。\n",
    "\n",
    "2. 进行计算，使用定点数张量进行计算。\n",
    "\n",
    "3. 计算后，将定点数张量反量化为浮点数张量。\n",
    "\n",
    "具体地说，在计算前，需要使用 quantize_per_tensor 函数将浮点数除以一个量化比例并四舍五入取整，然后加上一个量化零点得到一个整数。在计算过程中，使用定点数张量进行计算。计算后，需要使用 dequantize 函数将定点数张量减去量化零点，并乘以量化比例得到一个浮点数。\n",
    "\n",
    "所以，量化和反量化是成对使用的，用于将浮点数张量转换为定点数张量进行计算，然后将定点数张量转换回浮点数张量，以保证计算的正确性。\n",
    "\n",
    "\n",
    " **定义**：\n",
    "torch.quantize_per_tensor(input, scale, zero_point, dtype) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – float tensor or list of tensors to quantize  float张量或要量化的张量列表\n",
    "\n",
    "- scale (float or Tensor) – scale to apply in quantization formula  应用于量化公式的尺度\n",
    "\n",
    "- zero_point (int or Tensor) – offset in integer value that maps to float zero  映射到浮点零的整数值的偏移量\n",
    "\n",
    "- dtype (torch.dtype) – the desired data type of returned tensor. Has to be one of the quantized dtypes: torch.quint8, torch.qint8, torch.qint32  返回张量的期望数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7fdf2577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)\n",
    "# torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()\n",
    "# torch.quantize_per_tensor([torch.tensor([-1.0, 0.0]), torch.tensor([-2.0, 2.0])],\n",
    "# torch.tensor([0.1, 0.2]), torch.tensor([10, 20]), torch.quint8)\n",
    "# torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), torch.tensor(0.1), torch.tensor(10), torch.quint8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89e24ea4",
   "metadata": {},
   "source": [
    "## quantize_per_channel\n",
    "将一个浮点张量转换为具有给定尺度和零点的每通道量化张量。\n",
    "\n",
    " **定义**：\n",
    "torch.quantize_per_channel(input, scales, zero_points, axis, dtype) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – float tensor to quantize  float张量或要量化的张量列表\n",
    "\n",
    "- scales (Tensor) – float 1D tensor of scales to use, size should match input.size(axis)  应用于量化公式的尺度\n",
    "\n",
    "- zero_points (int) – integer 1D tensor of offset to use, size should match input.size(axis)  使用的偏移量的整数1D张量\n",
    "\n",
    "- axis (int) – dimension on which apply per-channel quantization  应用每个通道量化的维度\n",
    "\n",
    "- dtype (torch.dtype) – the desired data type of returned tensor. Has to be one of the quantized dtypes: torch.quint8, torch.qint8, torch.qint32  返回张量的期望数据类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "903f5961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  0.],\n",
       "        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_channel_affine,\n",
       "       scale=tensor([0.1000, 0.0100], dtype=torch.float64),\n",
       "       zero_point=tensor([10,  0]), axis=0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\n",
    "torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)\n",
    "# torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0717c42",
   "metadata": {},
   "source": [
    "## dequantize\n",
    "用于将定点数张量反量化为浮点数张量\n",
    "\n",
    "**定义**：\n",
    "torch.dequantize(tensor) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "96055592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2., 3.], size=(3,), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=20)\n",
      "y: tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个定点数张量\n",
    "x = torch.quantize_per_tensor(\n",
    "    torch.tensor([1., 2, 3]),\n",
    "    dtype=torch.quint8, scale=0.1, zero_point=20)\n",
    "\n",
    "# 将定点数张量反量化为浮点数张量\n",
    "y = torch.dequantize(x)\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bce7207",
   "metadata": {},
   "source": [
    "## complex\n",
    "构造一个复张量，其实部等于实部，虚部等于像。\n",
    "\n",
    "**定义**：\n",
    "torch.complex(real, imag, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- real (Tensor) – The real part of the complex tensor. Must be float or double.  复张量的实部。必须为float或double。\n",
    "\n",
    "- imag (Tensor) – The imaginary part of the complex tensor. Must be same dtype as real.  复张量的虚部。dtype必须和real相同。\n",
    "\n",
    "- out (Tensor) – If the inputs are torch.float32, must be torch.complex64. If the inputs are torch.float64, must be torch.complex128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "11480b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.+3.j, 2.+4.j])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "real = torch.tensor([1, 2], dtype=torch.float32)\n",
    "imag = torch.tensor([3, 4], dtype=torch.float32)\n",
    "z = torch.complex(real, imag)\n",
    "z\n",
    "# z.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af404828",
   "metadata": {},
   "source": [
    "## polar\n",
    "构造一个复张量，其元素为笛卡尔坐标，对应于绝对值为abs和角为angle的极坐标。\n",
    "\n",
    "**定义**：\n",
    "torch.polar(abs, angle, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- abs (Tensor) – The absolute value the complex tensor. Must be float or double.  复张量的绝对值\n",
    "\n",
    "- angle (Tensor) – The angle of the complex tensor. Must be same dtype as abs.  复张量的角度\n",
    "\n",
    "- out (Tensor) – If the inputs are torch.float32, must be torch.complex64. If the inputs are torch.float64, must be torch.complex128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ccbb149f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.1232e-17+1.0000j, -1.4142e+00-1.4142j], dtype=torch.complex128)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "abs = torch.tensor([1, 2], dtype=torch.float64)\n",
    "angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\n",
    "z = torch.polar(abs, angle)\n",
    "z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bebfce48",
   "metadata": {},
   "source": [
    "## heaviside\n",
    "为输入中的每个元素计算Heaviside阶跃函数。\n",
    "$$\\text{heaviside}(input,values)=\\begin{cases}0,&\\text{if }\\text{input}<0\\\\values,&\\text{if }\\mathrm{input}==0\\\\ 1,&\\text{if }\\operatorname{input}>0\\end{cases}$$\n",
    "\n",
    "**定义**：\n",
    "torch.heaviside(input, values, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.  输入张量\n",
    "\n",
    "- values (Tensor) – The values to use where input is zero.  用于输入为零的值\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1c26f0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., -2.,  1.])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.tensor([-1.5, 0, 2.0])\n",
    "values = torch.tensor([0.5])\n",
    "torch.heaviside(input, values)\n",
    "values = torch.tensor([1.2, -2.0, 3.5])\n",
    "torch.heaviside(input, values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f190754a",
   "metadata": {},
   "source": [
    "# 索引，切片，连接，变异操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a13f750",
   "metadata": {},
   "source": [
    "## adjoint\n",
    "返回共轭张量的视图，并将最后两个维度转置。\n",
    "\n",
    "**定义**：\n",
    "torch.adjoint(Tensor) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "41737cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.+0.j, 1.+1.j],\n",
      "        [2.+2.j, 3.+3.j]])\n",
      "tensor([[0.-0.j, 2.-2.j],\n",
      "        [1.-1.j, 3.-3.j]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4, dtype=torch.float)\n",
    "A = torch.complex(x, x).reshape(2, 2)\n",
    "print(A)\n",
    "print(A.adjoint())\n",
    "(A.adjoint() == A.mH).all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a93aa67",
   "metadata": {},
   "source": [
    "## argwhere\n",
    "返回一个张量，包含输入中所有非零元素的下标\n",
    "\n",
    "**定义**：\n",
    "torch.argwhere(input) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "187323be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 2],\n",
       "        [1, 1],\n",
       "        [1, 2]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.tensor([1, 0, 1])\n",
    "torch.argwhere(t)\n",
    "t = torch.tensor([[1, 0, 1], \n",
    "                  [0, 1, 1]])\n",
    "torch.argwhere(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6653c8c",
   "metadata": {},
   "source": [
    "## cat/concat/concatenate\n",
    "在给定维度中连接给定序列的seq张量。\n",
    "\n",
    "**定义**：\n",
    "torch.cat(tensors, dim=0, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "tensors (sequence of Tensors) – any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.  任何相同类型的python张量序列\n",
    "\n",
    "dim (int, optional) – the dimension over which the tensors are concatenated  连接张量的维度\n",
    "\n",
    "out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6938f4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8],\n",
       "        [0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8],\n",
       "        [0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(9).reshape(3,3)\n",
    "x\n",
    "torch.cat((x, x, x), 0)\n",
    "torch.concat((x, x, x), 0)\n",
    "torch.concatenate((x, x, x), 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b615011c",
   "metadata": {},
   "source": [
    "## conj\n",
    "用于计算输入张量的复共轭\n",
    "\n",
    "**定义**：\n",
    "torch.conj(input) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "78f99a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j])\n",
    "x.is_conj()\n",
    "y = torch.conj(x)\n",
    "y.is_conj()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa34286",
   "metadata": {},
   "source": [
    "## chunk\n",
    "尝试将一个张量分割为指定数量的块。每个块都是输入张量的一个视图。\n",
    "\n",
    "**定义**：\n",
    "torch.chunk(input, chunks, dim=0) → List of Tensors\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the tensor to split  要分割的张量\n",
    "\n",
    "- chunks (int) – number of chunks to return  返回的chunk个数\n",
    "\n",
    "- dim (int) – dimension along which to split the tensor  分割张量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a31825d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([4, 5, 6, 7]), tensor([ 8,  9, 10]))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.arange(11).chunk(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc62928f",
   "metadata": {},
   "source": [
    "## dsplit\n",
    "根据indices_or_sections将输入的三维或多维张量深度分割为多个张量。每个分割都是一个输入视图。\n",
    "\n",
    "**定义**：\n",
    "torch.dsplit(input, sections) → List of Tensors\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – tensor to split.  要分割的张量。\n",
    "\n",
    "- sections (int or list or tuple of ints) – See argument in torch.tensor_split().\n",
    "\n",
    "**图解**：\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/dsplit.svg\"\n",
    "    width=\"1000\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "703c6024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.],\n",
      "         [ 2.,  3.],\n",
      "         [ 4.,  5.],\n",
      "         [ 6.,  7.]],\n",
      "\n",
      "        [[ 8.,  9.],\n",
      "         [10., 11.],\n",
      "         [12., 13.],\n",
      "         [14., 15.]],\n",
      "\n",
      "        [[16., 17.],\n",
      "         [18., 19.],\n",
      "         [20., 21.],\n",
      "         [22., 23.]],\n",
      "\n",
      "        [[24., 25.],\n",
      "         [26., 27.],\n",
      "         [28., 29.],\n",
      "         [30., 31.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.],\n",
       "          [ 2.],\n",
       "          [ 4.],\n",
       "          [ 6.]],\n",
       " \n",
       "         [[ 8.],\n",
       "          [10.],\n",
       "          [12.],\n",
       "          [14.]],\n",
       " \n",
       "         [[16.],\n",
       "          [18.],\n",
       "          [20.],\n",
       "          [22.]],\n",
       " \n",
       "         [[24.],\n",
       "          [26.],\n",
       "          [28.],\n",
       "          [30.]]]),\n",
       " tensor([[[ 1.],\n",
       "          [ 3.],\n",
       "          [ 5.],\n",
       "          [ 7.]],\n",
       " \n",
       "         [[ 9.],\n",
       "          [11.],\n",
       "          [13.],\n",
       "          [15.]],\n",
       " \n",
       "         [[17.],\n",
       "          [19.],\n",
       "          [21.],\n",
       "          [23.]],\n",
       " \n",
       "         [[25.],\n",
       "          [27.],\n",
       "          [29.],\n",
       "          [31.]]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(32.0).reshape(4,4,2)\n",
    "print(t)\n",
    "torch.dsplit(t, sections=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f97fee1a",
   "metadata": {},
   "source": [
    "## column_stack\n",
    "通过水平叠加张量来创建一个新的张量。\n",
    "\n",
    "**定义**：\n",
    "torch.column_stack(tensors, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- tensors (sequence of Tensors) – sequence of tensors to concatenate  要连接的张量序列\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4eedf336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "torch.column_stack((a, b))\n",
    "# a = torch.arange(5)\n",
    "# b = torch.arange(10).reshape(5, 2)\n",
    "# torch.column_stack((a, b, b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72441051",
   "metadata": {},
   "source": [
    "## dstack\n",
    "按顺序深度堆叠张量(沿第三个轴)。\n",
    "\n",
    "**定义**：\n",
    "torch.dstack(tensors, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "tensors (sequence of Tensors) – sequence of tensors to concatenate  要连接的张量序列\n",
    "\n",
    "out (Tensor, optional) – the output tensor.  将创建的tensor赋值给out，共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "93051d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0],\n",
       "         [ 1, 10],\n",
       "         [ 2, 20]],\n",
       "\n",
       "        [[ 3, 30],\n",
       "         [ 4, 40],\n",
       "         [ 5, 50]],\n",
       "\n",
       "        [[ 6, 60],\n",
       "         [ 7, 70],\n",
       "         [ 8, 80]]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(9).reshape(3,3,1) #HWC\n",
    "b = torch.arange(9).reshape(3,3,1)*10\n",
    "a\n",
    "# a = torch.tensor([1, 2, 3])\n",
    "# b = torch.tensor([4, 5, 6])\n",
    "torch.dstack((a,b))\n",
    "# a = torch.tensor([[1],[2],[3]])\n",
    "# b = torch.tensor([[4],[5],[6]])\n",
    "# torch.dstack((a,b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "620cad66",
   "metadata": {},
   "source": [
    "## gather\n",
    "沿着dim指定的轴收集值。\n",
    "\n",
    "**定义**：\n",
    "torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "input (Tensor) – the source tensor\n",
    "\n",
    "dim (int) – the axis along which to index\n",
    "\n",
    "index (LongTensor) – the indices of elements to gather\n",
    "\n",
    "sparse_grad (bool, optional) – If True, gradient w.r.t. input will be a sparse tensor.\n",
    "\n",
    "out (Tensor, optional) – the destination tensor\n",
    "\n",
    "**图解**：\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/20230313191028.png\"\n",
    "    width=\"700\" /></p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/20230313191047.png\"\n",
    "    width=\"700\" /></p>\n",
    "\n",
    "https://machinelearningknowledge.ai/how-to-use-torch-gather-function-in-pytorch-with-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ea221c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 4, 8],\n",
      "        [6, 4, 2]])\n",
      "tensor([[2, 1, 0],\n",
      "        [3, 5, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.tensor([[0, 1, 2],\n",
    "                      [3, 4, 5],\n",
    "                      [6, 7, 8]]) \n",
    "\n",
    "print(torch.gather(input, dim=0, index=torch.tensor([[0, 1, 2],\n",
    "                                                     [2, 1, 0]])))\n",
    "\n",
    "print(torch.gather(input, dim=1, index=torch.tensor([[2, 1, 0],\n",
    "                                                     [0, 2, 1]])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ca294f4",
   "metadata": {},
   "source": [
    "## hsplit\n",
    "在水平方向上切分，相当于是竖着切\n",
    "\n",
    "**定义**：\n",
    "torch.hsplit(input, sections) → List of Tensors\n",
    "\n",
    "**参数**：\n",
    "input (Tensor) – tensor to split.\n",
    "\n",
    "indices_or_sections (int or list or tuple of ints) – See argument in torch.tensor_split().\n",
    "\n",
    "**图解**：\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/hsplit.svg\"\n",
    "    width=\"800\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e87519e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.],\n",
       "         [ 4.,  5.],\n",
       "         [ 8.,  9.],\n",
       "         [12., 13.]]),\n",
       " tensor([[ 2.,  3.],\n",
       "         [ 6.,  7.],\n",
       "         [10., 11.],\n",
       "         [14., 15.]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.arange(16.0).reshape(4,4)\n",
    "print(t)\n",
    "torch.hsplit(t, sections=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3e3daa2",
   "metadata": {},
   "source": [
    "## vsplit\n",
    "在竖直方向上切分，相当于是横着切\n",
    "\n",
    "**定义**：\n",
    "torch.vsplit(input, indices_or_sections) → List of Tensors\n",
    "\n",
    "**参数**：\n",
    "input (Tensor) – tensor to split.\n",
    "\n",
    "indices_or_sections (int or list or tuple of ints) – See argument in torch.tensor_split().\n",
    "\n",
    "**图解**：\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/hsplit.svg\"\n",
    "    width=\"800\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "c6634dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2., 3.],\n",
       "         [4., 5., 6., 7.]]),\n",
       " tensor([[ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]]))"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.arange(16.0).reshape(4,4)\n",
    "print(t)\n",
    "torch.vsplit(t, sections=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98560a10",
   "metadata": {},
   "source": [
    "## hstack\n",
    "水平方向上拼接\n",
    "\n",
    "**定义**：\n",
    "torch.hstack(tensors, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- tensors (sequence of Tensors) – sequence of tensors to concatenate\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b32849e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "torch.hstack((a, b))\n",
    "a = torch.tensor([[1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "b = torch.tensor([[4],\n",
    "                  [5],\n",
    "                  [6]])\n",
    "torch.hstack((a, b))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e4a5052",
   "metadata": {},
   "source": [
    "## vstack\n",
    "水平方向上拼接\n",
    "\n",
    "**定义**：\n",
    "torch.vstack(tensors, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- tensors (sequence of Tensors) – sequence of tensors to concatenate\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "231375cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "torch.vstack((a, b))\n",
    "a = torch.tensor([[1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "b = torch.tensor([[4],\n",
    "                  [5],\n",
    "                  [6]])\n",
    "torch.vstack((a, b))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e52a97b",
   "metadata": {},
   "source": [
    "## index_add\n",
    "按照索引给张量加减值。\n",
    "\n",
    "**定义**：\n",
    "Tensor.index_add_(dim, index, source, *, alpha=1) → Tensor\n",
    "\n",
    "\n",
    "**参数**：\n",
    "- dim (int) – dimension along which to index  索引的维度\n",
    "\n",
    "- index (Tensor) – indices of source to select from, should have dtype either torch.int64 or torch.int32  索引\n",
    "\n",
    "- source (Tensor) – the tensor containing values to add 要加减多少\n",
    "\n",
    "- alpha - 控制 src的倍数，即alpha * src  默认alpha=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cdf13427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  3.,  4.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 8.,  9., 10.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 5.,  6.,  7.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(5, 3)\n",
    "source = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]], dtype=torch.float)\n",
    "index = torch.tensor([0, 4, 2])\n",
    "x.index_add_(0, index, t)\n",
    "# x.index_add_(0, index, source, alpha=-1)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7074cdd4",
   "metadata": {},
   "source": [
    "## index_copy\n",
    "按照索引给张量拷贝值。\n",
    "\n",
    "**定义**：\n",
    "Tensor.index_copy_(dim, index, tensor) → Tensor\n",
    "\n",
    "\n",
    "**参数**：\n",
    "- dim (int) – dimension along which to index  索引的维度\n",
    "\n",
    "- index (LongTensor) – indices of tensor to select from  索引\n",
    "\n",
    "- tensor (Tensor) – the tensor containing values to copy  包含要复制的值的张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7baa6f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [7., 8., 9.],\n",
       "        [0., 0., 0.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.zeros(5, 3)\n",
    "t = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6], \n",
    "                  [7, 8, 9]], dtype=torch.float)\n",
    "index = torch.tensor([0, 4, 2])\n",
    "x.index_copy_(0, index, t)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eae87eb",
   "metadata": {},
   "source": [
    "## index_reduce\n",
    "\n",
    "\n",
    "**定义**：\n",
    "Tensor.index_reduce_(dim, index, source, reduce, *, include_self=True) → Tensor\n",
    "\n",
    "\n",
    "**参数**：\n",
    "- dim (int) – dimension along which to index  索引的维度\n",
    "\n",
    "- index (Tensor) – indices of source to select from, should have dtype either torch.int64 or torch.int32  索引\n",
    "\n",
    "- source (FloatTensor) – the tensor containing values to accumulate  包含要累积的值的张量\n",
    "\n",
    "- reduce (str) – the reduction operation to apply (\"prod\", \"mean\", \"amax\", \"amin\")  应用的reduction操作\n",
    "\n",
    "- include_self (bool) – whether the elements from the self tensor are included in the reduction\n",
    "self张量中的元素是否包含在reduction中\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "55874c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 22., 36.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [ 4.,  5.,  6.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.empty(5, 3).fill_(2)\n",
    "t = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6], \n",
    "                  [7, 8, 9], \n",
    "                  [10, 11, 12]], dtype=torch.float)\n",
    "index = torch.tensor([0, 4, 2, 0])\n",
    "x.index_reduce_(0, index, t, 'prod')\n",
    "# x = torch.empty(5, 3).fill_(1)\n",
    "x.index_reduce_(0, index, t, 'prod', include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2124bdfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20., 44., 72.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [14., 16., 18.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [ 8., 10., 12.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.empty(5, 3).fill_(2)\n",
    "t = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6], \n",
    "                  [7, 8, 9], \n",
    "                  [10, 11, 12]], dtype=torch.float)\n",
    "index = torch.tensor([0, 4, 2, 0])\n",
    "x.index_reduce_(0, index, t, 'prod')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c1464a1",
   "metadata": {},
   "source": [
    "## index_select\n",
    "返回一个新的张量，该张量使用索引中的项(LongTensor)沿dim维度索引输入张量。\n",
    "\n",
    "**定义**：\n",
    "torch.index_select(input, dim, index, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.  输入张量\n",
    "\n",
    "- dim (int) – the dimension in which we index  索引\n",
    "\n",
    "- index (IntTensor or LongTensor) – the 1-D tensor containing the indices to index  包含索引索引的一维张量\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  输出张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "65f57122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3960, -0.3405,  1.9604,  0.6839],\n",
      "        [-1.1725,  0.6119, -3.0097, -0.2486],\n",
      "        [ 0.8733,  0.0302, -0.0827, -0.1217]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3960,  1.9604],\n",
       "        [-1.1725, -3.0097],\n",
       "        [ 0.8733, -0.0827]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "indices = torch.tensor([0, 2])\n",
    "torch.index_select(x, 0, indices)\n",
    "torch.index_select(x, 1, indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cce384de",
   "metadata": {},
   "source": [
    "## masked_select\n",
    "返回一个新的一维张量，根据布尔掩码mask (boolean tensor)对输入张量进行索引。\n",
    "\n",
    "**定义**：\n",
    "torch.masked_select(input, mask, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.\n",
    "\n",
    "- mask (BoolTensor) – the tensor containing the binary mask to index with\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  输出张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bdcca2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1306, -0.4987,  0.2276, -0.5090],\n",
      "        [-0.1216,  1.5538,  0.6814,  0.6978],\n",
      "        [-0.2731, -1.4790,  0.8621,  0.6601]])\n",
      "tensor([[ True, False, False, False],\n",
      "        [False,  True,  True,  True],\n",
      "        [False, False,  True,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.1306, 1.5538, 0.6814, 0.6978, 0.8621, 0.6601])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "mask = x.ge(0.5)\n",
    "print(mask)\n",
    "torch.masked_select(x, mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae2e491f",
   "metadata": {},
   "source": [
    "## movedim/moveaxis\n",
    "将输入的维度从源的位置移动到目标的位置。\n",
    "\n",
    "**定义**：\n",
    "torch.movedim(input, source, destination) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.  输入张量\n",
    "\n",
    "- source (int or tuple of ints) – Original positions of the dims to move. These must be unique.  原始位置\n",
    "\n",
    "- destination (int or tuple of ints) – Destination positions for each of the original dims. These must also be unique.  目标位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "041c518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0],\n",
      "         [1]],\n",
      "\n",
      "        [[2],\n",
      "         [3]],\n",
      "\n",
      "        [[4],\n",
      "         [5]]])\n",
      "tensor([[[0],\n",
      "         [2],\n",
      "         [4]],\n",
      "\n",
      "        [[1],\n",
      "         [3],\n",
      "         [5]]])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.arange(6).reshape(3,2,1)\n",
    "print(t)\n",
    "\n",
    "print(torch.movedim(t, 1, 0))\n",
    "print(torch.movedim(t, 1, 0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b9ef4a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0],\n",
      "         [1]],\n",
      "\n",
      "        [[2],\n",
      "         [3]],\n",
      "\n",
      "        [[4],\n",
      "         [5]]])\n",
      "torch.Size([2, 1, 3])\n",
      "tensor([[[0, 2, 4]],\n",
      "\n",
      "        [[1, 3, 5]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.arange(6).reshape(3,2,1)\n",
    "print(t)\n",
    "print(torch.movedim(t, (1, 2), (0, 1)).shape) #1换0， 2换1\n",
    "print(torch.movedim(t, (1, 2), (0, 1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "facf611f",
   "metadata": {},
   "source": [
    "## narrow\n",
    "返回一个新的张量，它是输入张量的缩小版本。尺寸dim是从开始到开始的输入+长度。返回的张量和输入张量共享相同的底层存储空间。\n",
    "\n",
    "**定义**：\n",
    "torch.narrow(input, dim, start, length) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the tensor to narrow   要窄化的张量\n",
    "\n",
    "- dim (int) – the dimension along which to narrow  沿着哪个维度窄化\n",
    "\n",
    "- start (Tensor or int) – the starting dimension  起始位置\n",
    "\n",
    "- length (int) – the distance to the ending dimension  长度\n",
    "\n",
    "**图解**：\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/narrow.svg\"\n",
    "    width=\"1000\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8f496bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(16.0).reshape(4, 4)\n",
    "print(x)\n",
    "torch.narrow(x, dim=0, start=1, length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d931c0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  3.],\n",
       "        [ 6.,  7.],\n",
       "        [10., 11.],\n",
       "        [14., 15.]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(16.0).reshape(4, 4)\n",
    "print(x)\n",
    "torch.narrow(x, dim=1, start=2, length=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec62877e",
   "metadata": {},
   "source": [
    "## nonzero\n",
    "返回一个张量，包含输入中所有非零元素的下标。\n",
    "\n",
    "**定义**：\n",
    "torch.nonzero(input, *, out=None, as_tuple=False) → LongTensor or tuple of LongTensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "afaf3a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2],\n",
       "        [3, 3]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\n",
    "torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
    "                            [0.0, 0.4, 0.0, 0.0],\n",
    "                            [0.0, 0.0, 1.2, 0.0],\n",
    "                            [0.0, 0.0, 0.0,-0.4]]))\n",
    "# torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n",
    "# torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
    "#                             [0.0, 0.4, 0.0, 0.0],\n",
    "#                             [0.0, 0.0, 1.2, 0.0],\n",
    "#                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\n",
    "# # torch.nonzero(torch.tensor(5), as_tuple=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce8dc82f",
   "metadata": {},
   "source": [
    "## permute\n",
    "函数通过重排张量的轴顺序来实现维度变换，它不会改变张量中元素的顺序\n",
    "\n",
    "**定义**：\n",
    "torch.permute(input, dims) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.  输入张量\n",
    "\n",
    "- dims (tuple of python:int) – The desired ordering of dimensions  所需的维度顺序\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b76e025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0],\n",
      "         [1]],\n",
      "\n",
      "        [[2],\n",
      "         [3]],\n",
      "\n",
      "        [[4],\n",
      "         [5]]])\n",
      "torch.Size([3, 2, 1])\n",
      "===========\n",
      "tensor([[[0],\n",
      "         [2],\n",
      "         [4]],\n",
      "\n",
      "        [[1],\n",
      "         [3],\n",
      "         [5]]])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(6).reshape(3,2,1)\n",
    "print(x)\n",
    "print(x.size())\n",
    "print(\"===========\")\n",
    "y= torch.permute(x, (1, 0, 2)) #可以对所有的轴进行重排序\n",
    "print(y)\n",
    "print(y.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "071d873e",
   "metadata": {},
   "source": [
    "## reshape\n",
    "返回一个张量，其数据和元素数量与输入相同，但具有指定的形状。如果可能的话，返回的张量将是输入的视图。否则，它将是一个副本。\n",
    "\n",
    "**定义**：\n",
    "torch.reshape(input, shape) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the tensor to be reshaped  需要重塑的张量\n",
    "\n",
    "- shape (tuple of python:int) – the new shape  新的形状\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ce7008dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(4.)\n",
    "print(a)\n",
    "torch.reshape(a, (2, 2))\n",
    "# b = torch.tensor([[0, 1], [2, 3]])\n",
    "# torch.reshape(b, (-1,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1891957",
   "metadata": {},
   "source": [
    "## vstack/row_stack\n",
    "按竖直顺序(按行)堆叠张量。\n",
    "\n",
    "**定义**：\n",
    "torch.vstack(tensors, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- tensors (sequence of Tensors) – sequence of tensors to concatenate  要连接的张量序列\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.  输出张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f9877f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "torch.vstack((a,b))\n",
    "# a = torch.tensor([[1],[2],[3]])\n",
    "# b = torch.tensor([[4],[5],[6]])\n",
    "# torch.vstack((a,b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f11c14a0",
   "metadata": {},
   "source": [
    "## select\n",
    "在给定的索引处沿选定的维度对输入张量进行切片。这个函数返回去掉给定维度后原始张量的视图。\n",
    "select() is equivalent to slicing.\n",
    "\n",
    "**定义**：\n",
    "torch.select(input, dim, index) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.\n",
    "\n",
    "- dim (int) – the dimension to slice\n",
    "\n",
    "- index (int) – the index to select with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2ddf04f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]])\n",
      "torch.Size([4])\n",
      "tensor([ 1.,  5.,  9., 13.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(16.0).reshape(4, 4)\n",
    "print(x)\n",
    "\n",
    "selected = torch.select(x, dim=1, index=1)\n",
    "\n",
    "print(selected.shape)\n",
    "print(selected)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf2d7a0e",
   "metadata": {},
   "source": [
    "## scatter\n",
    "在索引张量中指定的索引处，将张量src中的所有值写入\n",
    "\n",
    "**定义**：\n",
    "torch.scatter(input, dim, index, src) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- dim (int) – the axis along which to index\n",
    "\n",
    "- index (LongTensor) – the indices of elements to scatter, can be either empty or of the same dimensionality as src. When empty, the operation returns self unchanged.\n",
    "\n",
    "- src (Tensor or float) – the source element(s) to scatter.\n",
    "\n",
    "- reduce (str, optional) – reduction operation to apply, can be either 'add' or 'multiply'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "817a4da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 4, 0],\n",
       "        [0, 2, 0, 0, 0],\n",
       "        [0, 0, 3, 0, 0]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "src = torch.arange(1, 11).reshape((2, 5))\n",
    "print(src)\n",
    "index = torch.tensor([[0, 1, 2, 0]])\n",
    "torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ea85cfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 0, 0],\n",
       "        [6, 7, 0, 0, 8],\n",
       "        [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "src = torch.arange(1, 11).reshape((2, 5))\n",
    "print(src)\n",
    "index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n",
    "torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "92b624aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n",
       "        [2.0000, 2.0000, 2.0000, 2.4600]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "src = torch.arange(1, 11).reshape((2, 5))\n",
    "print(src)\n",
    "\n",
    "torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]), 1.23, reduce='multiply')\n",
    "# torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
    "#            1.23, reduce='add')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58647552",
   "metadata": {},
   "source": [
    "## diagonal_scatter\n",
    "将src张量的值沿输入的对角线元素嵌入到输入中，相对于dim1和dim2。\n",
    "\n",
    "**定义**：\n",
    "torch.diagonal_scatter(input, src, offset=0, dim1=0, dim2=1) → Tensor\n",
    "\n",
    "**参数**：\n",
    "input (Tensor) – the input tensor. Must be at least 2-dimensional.  输入张量。至少是二维的。\n",
    "\n",
    "src (Tensor) – the tensor to embed into input.  要嵌入到输入中的张量。\n",
    "\n",
    "offset (int, optional) – which diagonal to consider. Default: 0 (main diagonal).  考虑哪条对角线。默认值:0(主对角线)。\n",
    "\n",
    "dim1 (int, optional) – first dimension with respect to which to take diagonal. Default: 0.  对角线的第一个维度。默认值:0。\n",
    "\n",
    "dim2 (int, optional) – second dimension with respect to which to take diagonal. Default: 1.  对其进行对角线处理的第二个维度。默认值:1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6cfcdfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 10., 10.],\n",
       "        [10.,  1., 10.],\n",
       "        [10., 10.,  1.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.zeros(3, 3)+10\n",
    "a\n",
    "torch.diagonal_scatter(a, torch.ones(3))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "128a6188",
   "metadata": {},
   "source": [
    "## select_scatter\n",
    "将src张量的值嵌入到给定索引处的输入中。这个函数返回一个有新存储空间的张量;它不创建视图。\n",
    "\n",
    "**定义**：\n",
    "torch.select_scatter(input, src, dim, index) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.  输入张量\n",
    "\n",
    "- src (Tensor) – The tensor to embed into input  要嵌入到输入中的张量\n",
    "\n",
    "- dim (int) – the dimension to insert the slice into.  插入切片的维度\n",
    "\n",
    "- index (int) – the index to select with  选择的索引\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e03966dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 5, 6],\n",
       "        [1, 8, 9]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9],\n",
    "                  ])\n",
    "src = torch.ones(3)\n",
    "a.select_scatter(src = src, dim = 1, index = 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bac7413",
   "metadata": {},
   "source": [
    "## slice_scatter\n",
    "将src张量的值嵌入到给定维度的输入中。这个函数返回一个有新存储空间的张量;它不创建视图。\n",
    "\n",
    "**定义**：\n",
    "torch.slice_scatter(input, src, dim=0, start=None, end=None, step=1) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.  输入张量\n",
    "\n",
    "- src (Tensor) – The tensor to embed into input  要嵌入到输入中的张量\n",
    "\n",
    "- dim (int) – the dimension to insert the slice into  插入切片的维度\n",
    "\n",
    "- start (Optional[int]) – the start index of where to insert the slice  开始位置\n",
    "\n",
    "- end (Optional[int]) – the end index of where to insert the slice  结束为止\n",
    "\n",
    "- step (int) – the how many elements to skip in  步长\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f6c9d66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.zeros(8, 8)\n",
    "src = torch.ones(3,8)\n",
    "a.slice_scatter(src, start=0,end=6,step=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0109bb44",
   "metadata": {},
   "source": [
    "## scatter_add\n",
    "将张量src中的所有值添加到self中\n",
    "\n",
    "**定义**：\n",
    "torch.scatter_add(input, dim, index, src) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- dim (int) – the axis along which to index  索引的轴\n",
    "\n",
    "- index (LongTensor) – the indices of elements to scatter and add, can be either empty or of the same dimensionality as src. When empty, the operation returns self unchanged.\n",
    "\n",
    "- src (Tensor) – the source elements to scatter and add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c484a6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101., 100., 100., 101., 101.],\n",
       "        [100., 101., 100., 100., 100.],\n",
       "        [100., 100., 101., 100., 100.]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "src = torch.ones((2, 5))\n",
    "index = torch.tensor([[0, 1, 2, 0, 0]])\n",
    "(torch.ones(3, 5, dtype=src.dtype)*100).scatter_add_(0, index, src)\n",
    "# index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n",
    "# torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8e987c5",
   "metadata": {},
   "source": [
    "## scatter_reduce\n",
    "将src张量中的所有值reduce到self张量中\n",
    "\n",
    "**定义**：\n",
    "torch.scatter_reduce(input, dim, index, src, reduce, *, include_self=True) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- dim (int) – the axis along which to index  索引的轴\n",
    "\n",
    "- index (LongTensor) – the indices of elements to scatter and reduce.\n",
    "\n",
    "- src (Tensor) – the source elements to scatter and reduce\n",
    "\n",
    "- reduce (str) – the reduction operation to apply for non-unique indices (\"sum\", \"prod\", \"mean\", \"amax\", \"amin\")\n",
    "\n",
    "- include_self (bool) – whether elements from the self tensor are included in the reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "93236962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13., 32., 35., 40., 50., 61., 70.])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.tensor([1., 2., 3., 4., 5, 6, 7])*10\n",
    "src = torch.tensor([1., 2., 3., 4., 5., 6.])\n",
    "index = torch.tensor([5, 1, 0, 1, 2, 1])\n",
    "input.scatter_reduce(0, index, src, reduce=\"sum\")  # input[5]+=src[0] 即60+1=61\n",
    "# input.scatter_reduce(0, index, src, reduce=\"sum\", include_self=False)\n",
    "# input2 = torch.tensor([5., 4., 3., 2.])\n",
    "# input2.scatter_reduce(0, index, src, reduce=\"amax\")\n",
    "# input2.scatter_reduce(0, index, src, reduce=\"amax\", include_self=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65bf128f",
   "metadata": {},
   "source": [
    "## split\n",
    "把张量分成若干块。每个块都是原始张量的一个视图。\n",
    "\n",
    "**定义**：\n",
    "torch.split(tensor, split_size_or_sections, dim=0)\n",
    "\n",
    "**参数**：\n",
    "- tensor (Tensor) – tensor to split.\n",
    "\n",
    "- split_size_or_sections (int) or (list(int)) – size of a single chunk or list of sizes for each chunk  单个块的大小或每个块的大小列表\n",
    "\n",
    "- dim (int) – dimension along which to split the tensor.  分割张量的维度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "aa5239d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [2, 3]]),\n",
       " tensor([[4, 5],\n",
       "         [6, 7]]),\n",
       " tensor([[8, 9]]))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(10).reshape(5,2)\n",
    "print(a)\n",
    "torch.split(a, split_size_or_sections=2)\n",
    "# torch.split(a, [1,4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9aec686",
   "metadata": {},
   "source": [
    "## squeeze\n",
    "返回一个删除了所有大小为1的输入维度的张量。\n",
    "\n",
    "**定义**：\n",
    "torch.squeeze(input, dim=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "input (Tensor) – the input tensor.\n",
    "\n",
    "dim (int, optional) – if given, the input will be squeezed only in this dimension  如果给定，输入将只压缩到这个维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6d60d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.zeros(2, 1, 2, 1, 2)\n",
    "print(x.size())\n",
    "y1 = torch.squeeze(x)\n",
    "print(y1.size())\n",
    "y2 = torch.squeeze(x, 0)\n",
    "print(y2.size())\n",
    "y3 = torch.squeeze(x, 1)\n",
    "print(y3.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e38a0477",
   "metadata": {},
   "source": [
    "## stack\n",
    "沿着一个新的维度连接一个张量序列。所有张量的大小必须相同。\n",
    "\n",
    "**定义**：\n",
    "torch.stack(tensors, dim=0, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- tensors (sequence of Tensors) – sequence of tensors to concatenate\n",
    "\n",
    "- dim (int) – dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive)\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6c810f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "print(torch.stack((a,b),dim=0))\n",
    "print(torch.stack((a,b),dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d5272be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[   0,    1,    2,    3],\n",
      "         [   4,    5,    6,    7],\n",
      "         [   8,    9,   10,   11],\n",
      "         [  12,   13,   14,   15]],\n",
      "\n",
      "        [[   0,  100,  200,  300],\n",
      "         [ 400,  500,  600,  700],\n",
      "         [ 800,  900, 1000, 1100],\n",
      "         [1200, 1300, 1400, 1500]]])\n",
      "tensor([[[   0,    1,    2,    3],\n",
      "         [   0,  100,  200,  300]],\n",
      "\n",
      "        [[   4,    5,    6,    7],\n",
      "         [ 400,  500,  600,  700]],\n",
      "\n",
      "        [[   8,    9,   10,   11],\n",
      "         [ 800,  900, 1000, 1100]],\n",
      "\n",
      "        [[  12,   13,   14,   15],\n",
      "         [1200, 1300, 1400, 1500]]])\n",
      "tensor([[[   0,    0],\n",
      "         [   1,  100],\n",
      "         [   2,  200],\n",
      "         [   3,  300]],\n",
      "\n",
      "        [[   4,  400],\n",
      "         [   5,  500],\n",
      "         [   6,  600],\n",
      "         [   7,  700]],\n",
      "\n",
      "        [[   8,  800],\n",
      "         [   9,  900],\n",
      "         [  10, 1000],\n",
      "         [  11, 1100]],\n",
      "\n",
      "        [[  12, 1200],\n",
      "         [  13, 1300],\n",
      "         [  14, 1400],\n",
      "         [  15, 1500]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(16).reshape(4,4)\n",
    "b = torch.arange(16).reshape(4,4)*100\n",
    "print(torch.stack((a,b),dim=0))\n",
    "print(torch.stack((a,b),dim=1))\n",
    "print(torch.stack((a,b),dim=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abcdfa70",
   "metadata": {},
   "source": [
    "## transpose/swapaxes/swapdims/t\n",
    "返回一个张量，它是输入的转置版本。给定的维度dim0和dim1被交换。\n",
    "\n",
    "**定义**：\n",
    "torch.transpose(input, dim0, dim1) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.  输入张量\n",
    "\n",
    "- dim0 (int) – the first dimension to be transposed  要转置的第一个维度\n",
    "\n",
    "- dim1 (int) – the second dimension to be transposed  要转置的第二个维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d90d3330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12],\n",
       "        [ 1,  5,  9, 13],\n",
       "        [ 2,  6, 10, 14],\n",
       "        [ 3,  7, 11, 15]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(16).reshape(4,4)\n",
    "print(x)\n",
    "torch.transpose(x, 0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70fe9a42",
   "metadata": {},
   "source": [
    "## take\n",
    "返回一个新的张量，其中包含给定下标处的输入元素。输入张量被看作是一个一维张量。\n",
    "\n",
    "**定义**：\n",
    "torch.take(input, index) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.\n",
    "\n",
    "- index (LongTensor) – the indices into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "31282fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 8])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "src = torch.tensor([[4, 3, 5],\n",
    "                    [6, 7, 8]])\n",
    "torch.take(src, torch.tensor([0, 2, 5]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b675bbdb",
   "metadata": {},
   "source": [
    "## take_along_dim\n",
    "从给定dim沿线的索引中选择1维索引处的输入值。\n",
    "\n",
    "**定义**：\n",
    "torch.take_along_dim(input, indices, dim, *, out=None) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.\n",
    "\n",
    "- indices (tensor) – the indices into input. Must have long dtype.\n",
    "\n",
    "- dim (int) – dimension to select along.\n",
    "\n",
    "- out (Tensor, optional) – the output tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "940172c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([60])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.tensor([[10, 30, 20], \n",
    "                  [60, 40, 50]])\n",
    "max_idx = torch.argmax(t)\n",
    "print(max_idx)\n",
    "torch.take_along_dim(t, max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "01709a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 1],\n",
      "        [1, 2, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10, 20, 30],\n",
       "        [40, 50, 60]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.tensor([[10, 30, 20], \n",
    "                  [60, 40, 50]])\n",
    "sorted_idx = torch.argsort(t, dim=1)\n",
    "print(sorted_idx)\n",
    "torch.take_along_dim(t, sorted_idx, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f0873c0",
   "metadata": {},
   "source": [
    "## tensor_split\n",
    "\n",
    "\n",
    "**定义**：\n",
    "torch.tensor_split(input, indices_or_sections, dim=0) → List of Tensors\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the tensor to split  要分割的张量\n",
    "\n",
    "- indices_or_sections (Tensor, int or list or tuple of ints) – 分成几个部分\n",
    "\n",
    "- dim (int, optional) – dimension along which to split the tensor. Default: 0  分割张量的维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "294ef7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [2, 3],\n",
       "         [4, 5]]),\n",
       " tensor([[6, 7],\n",
       "         [8, 9]]))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(10).reshape(5,2)\n",
    "print(a)\n",
    "torch.tensor_split(a, sections=2)\n",
    "# torch.split(a, [1,4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5414a2cd",
   "metadata": {},
   "source": [
    "## tile\n",
    "通过重复输入元素来构造一个张量。参数dims指定每个维度中的重复次数。\n",
    "\n",
    "**定义**：\n",
    "torch.tile(input, dims) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the tensor whose elements to repeat.  需要重复其元素的张量。\n",
    "\n",
    "- dims (tuple) – the number of repetitions per dimension.  每个维度的重复次数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "58e2af28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 1, 2, 3])\n",
      "tensor([[1, 2, 1, 2],\n",
      "        [3, 4, 3, 4],\n",
      "        [1, 2, 1, 2],\n",
      "        [3, 4, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(x.tile((2,)))\n",
    "y = torch.tensor([[1, 2], \n",
    "                  [3, 4]])\n",
    "print(torch.tile(y, (2, 2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cae02883",
   "metadata": {},
   "source": [
    "## unbind\n",
    "移除一个张量维数。\n",
    "\n",
    "**定义**：\n",
    "torch.unbind(input, dim=0) → seq\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the tensor to unbind\n",
    "\n",
    "- dim (int) – dimension to remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c536fe14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.unbind(torch.tensor([[1, 2, 3],\n",
    "                           [4, 5, 6],\n",
    "                           [7, 8, 9]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "787d7482",
   "metadata": {},
   "source": [
    "## unsqueeze\n",
    "返回一个插入到指定位置的新张量，其维数为1。\n",
    "\n",
    "**定义**：\n",
    "torch.unsqueeze(input, dim) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- input (Tensor) – the input tensor.  输入张量\n",
    "\n",
    "- dim (int) – the index at which to insert the singleton dimension  插入单维度的索引\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b5259ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "torch.unsqueeze(x, 0)\n",
    "# torch.unsqueeze(x, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3811195e",
   "metadata": {},
   "source": [
    "## where\n",
    "返回从x或y中选择的元素的张量，这取决于条件。\n",
    "\n",
    "$$\\operatorname{out}_i=\\begin{cases}\\mathrm x_i&\\text{if condition}_i\\\\ \\mathrm y_i&\\text{otherwise}\\end{cases}$$\n",
    "\n",
    "**定义**：\n",
    "torch.where(condition, x, y) → Tensor\n",
    "\n",
    "**参数**：\n",
    "- condition (BoolTensor) – When True (nonzero), yield x, otherwise yield y\n",
    "\n",
    "- x (Tensor or Scalar) – value (if x is a scalar) or values selected at indices where condition is True\n",
    "\n",
    "- y (Tensor or Scalar) – value (if y is a scalar) or values selected at indices where condition is False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8e201783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000],\n",
       "        [1.2312, 1.0000],\n",
       "        [1.0000, 0.2993]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3, 2)\n",
    "y = torch.ones(3, 2)\n",
    "x\n",
    "torch.where(x > 0, x, y)\n",
    "# x = torch.randn(2, 2, dtype=torch.double)\n",
    "# x\n",
    "# torch.where(x > 0, x, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c21134",
   "metadata": {},
   "source": [
    "## xxx\n",
    "\n",
    "\n",
    "**定义**：\n",
    "\n",
    "\n",
    "**参数**：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32680d43",
   "metadata": {},
   "source": [
    "## xxx\n",
    "\n",
    "\n",
    "**定义**：\n",
    "\n",
    "\n",
    "**参数**：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8da99",
   "metadata": {},
   "source": [
    "## xxx\n",
    "\n",
    "\n",
    "**定义**：\n",
    "\n",
    "\n",
    "**参数**：\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e00c79739f2fdf113306667eb0b8e68d4274855301e6df90bc305a954991b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
