{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36e625b5",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/itmorn/AI.handbook/blob/main/DL/torch/nn/Sparse/Embedding.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14a8662b",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "A simple lookup table that stores embeddings of a fixed dictionary and size.    \n",
    "一个简单的查找表，存储固定字典和大小的嵌入。\n",
    "\n",
    "This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.  \n",
    "这个模块通常用于存储词嵌入并使用索引检索它们。模块的输入是索引列表，输出是相应的词嵌入。\n",
    "\n",
    "**定义**：   \n",
    "torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)\n",
    "\n",
    "**参数**：  \n",
    "- num_embeddings (int) – size of the dictionary of embeddings  嵌入字典的大小\n",
    "\n",
    "- embedding_dim (int) – the size of each embedding vector  每个嵌入向量的大小\n",
    "\n",
    "- padding_idx (int, optional) – If specified, the entries at padding_idx do not contribute to the gradient; therefore, the embedding vector at padding_idx is not updated during training, i.e. it remains as a fixed “pad”. For a newly constructed Embedding, the embedding vector at padding_idx will default to all zeros, but can be updated to another value to be used as the padding vector.  如果指定了，padding_idx上的条目不参与梯度;因此，在padding_idx处的嵌入向量在训练过程中不会更新，即它仍然是一个固定的“pad”。对于新构造的Embedding, padding_idx处的嵌入向量将默认为全零，但可以更新为另一个值用作填充向量。\n",
    "\n",
    "- max_norm (float, optional) – If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm.  如果给定，每个模数大于max_norm的嵌入向量将被重规范化为模数max_norm。\n",
    "\n",
    "- norm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2.  用于max_norm选项计算的p-norm的p。默认2。\n",
    "\n",
    "- scale_grad_by_freq (bool, optional) – If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False.  如果给出，这将按迷你批处理中单词频率的倒数缩放梯度。\n",
    "\n",
    "- sparse (bool, optional) – If True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.  如果为True，梯度w.r.t.权重矩阵将是一个稀疏张量。有关稀疏梯度的详细信息，请参阅注释。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38fd4b85",
   "metadata": {},
   "source": [
    "# 图解建表查表\n",
    "    \n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/Embedding.svg\"\n",
    "    width=\"1000\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a5a4e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " tensor([[1, 2, 4, 5],\n",
      "        [4, 3, 2, 9]]) \n",
      "\n",
      "embedding.weight:\n",
      " Parameter containing:\n",
      "tensor([[-0.7747,  0.7926, -0.0062],\n",
      "        [-0.4377,  0.4657, -0.1880],\n",
      "        [-0.8975,  0.4169, -0.3840],\n",
      "        [ 0.0394,  0.4869, -0.1476],\n",
      "        [-0.4459, -0.0336, -0.6461],\n",
      "        [ 0.3470,  0.8133, -0.8232],\n",
      "        [ 0.7238,  1.3477,  0.9699],\n",
      "        [-1.0729,  0.4506,  0.0600],\n",
      "        [-0.2728,  0.0554,  1.9797],\n",
      "        [ 0.2763,  0.3080, -0.2687]], requires_grad=True) \n",
      "\n",
      "output:\n",
      " tensor([[[-0.4377,  0.4657, -0.1880],\n",
      "         [-0.8975,  0.4169, -0.3840],\n",
      "         [-0.4459, -0.0336, -0.6461],\n",
      "         [ 0.3470,  0.8133, -0.8232]],\n",
      "\n",
      "        [[-0.4459, -0.0336, -0.6461],\n",
      "         [ 0.0394,  0.4869, -0.1476],\n",
      "         [-0.8975,  0.4169, -0.3840],\n",
      "         [ 0.2763,  0.3080, -0.2687]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "embedding.weight:\n",
      " Parameter containing:\n",
      "tensor([[-0.7747,  0.7926, -0.0062],\n",
      "        [-0.5377,  0.3657, -0.2880],\n",
      "        [-0.9975,  0.3169, -0.4840],\n",
      "        [-0.0606,  0.3869, -0.2476],\n",
      "        [-0.5459, -0.1336, -0.7461],\n",
      "        [ 0.2470,  0.7133, -0.9232],\n",
      "        [ 0.7238,  1.3477,  0.9699],\n",
      "        [-1.0729,  0.4506,  0.0600],\n",
      "        [-0.2728,  0.0554,  1.9797],\n",
      "        [ 0.1763,  0.2080, -0.3687]], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调包计算\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "input = torch.LongTensor([[1, 2, 4, 5],\n",
    "                          [4, 3, 2, 9]])\n",
    "print(\"input:\\n\", input, \"\\n\")\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
    "print(\"embedding.weight:\\n\", embedding.weight, \"\\n\")\n",
    "\n",
    "output = embedding(input)\n",
    "print(\"output:\\n\", output, \"\\n\")\n",
    "\n",
    "loss = output.sum()\n",
    "optimizer = torch.optim.Adam(embedding.parameters(), lr=0.1) \n",
    "loss.backward()\n",
    "optimizer.step() # 反向传播会更新embedding.weight\n",
    "\n",
    "print(\"embedding.weight:\\n\", embedding.weight, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e02a3feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " tensor([[0, 2, 0, 5]]) \n",
      "\n",
      "embedding.weight:\n",
      " Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.4377,  0.4657, -0.1880],\n",
      "        [-0.8975,  0.4169, -0.3840],\n",
      "        [ 0.0394,  0.4869, -0.1476],\n",
      "        [-0.4459, -0.0336, -0.6461],\n",
      "        [ 0.3470,  0.8133, -0.8232],\n",
      "        [ 0.7238,  1.3477,  0.9699],\n",
      "        [-1.0729,  0.4506,  0.0600],\n",
      "        [-0.2728,  0.0554,  1.9797],\n",
      "        [ 0.2763,  0.3080, -0.2687]], requires_grad=True) \n",
      "\n",
      "output:\n",
      " tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.8975,  0.4169, -0.3840],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3470,  0.8133, -0.8232]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "embedding.weight:\n",
      " Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.4377,  0.4657, -0.1880],\n",
      "        [-0.9975,  0.3169, -0.4840],\n",
      "        [ 0.0394,  0.4869, -0.1476],\n",
      "        [-0.4459, -0.0336, -0.6461],\n",
      "        [ 0.2470,  0.7133, -0.9232],\n",
      "        [ 0.7238,  1.3477,  0.9699],\n",
      "        [-1.0729,  0.4506,  0.0600],\n",
      "        [-0.2728,  0.0554,  1.9797],\n",
      "        [ 0.2763,  0.3080, -0.2687]], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调包计算\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "input = torch.LongTensor([[0,2,0,5]])\n",
    "print(\"input:\\n\", input, \"\\n\")\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3, padding_idx=0)\n",
    "\n",
    "print(\"embedding.weight:\\n\", embedding.weight, \"\\n\")\n",
    "\n",
    "output = embedding(input)\n",
    "print(\"output:\\n\", output, \"\\n\")\n",
    "\n",
    "loss = output.sum()\n",
    "optimizer = torch.optim.Adam(embedding.parameters(), lr=0.1) \n",
    "loss.backward()\n",
    "optimizer.step()  # 反向传播不会更新padding_idx对应的向量\n",
    "\n",
    "print(\"embedding.weight:\\n\", embedding.weight, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e00c79739f2fdf113306667eb0b8e68d4274855301e6df90bc305a954991b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
