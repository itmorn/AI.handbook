{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c202a11",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/itmorn/AI.handbook/blob/main/DL/torch/nn/Transformer/TransformerEncoderLayer.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14a8662b",
   "metadata": {},
   "source": [
    "# TransformerEncoderLayer\n",
    "TransformerEncoderLayer是Transformer模型的组成部分之一，用于处理输入序列的编码器层。它由多个子层组成，包括多头自注意力层（Multi-Head Attention Layer）、前馈全连接层（Feed-Forward Layer）和残差连接（Residual Connection）等。\n",
    "\n",
    "具体来说，TransformerEncoderLayer将输入序列作为其输入，并将其经过多头自注意力层进行编码。在自注意力层中，每个位置的编码向量会同时参与计算所有位置的编码向量，从而捕捉序列中的全局信息。然后，编码向量会通过前馈全连接层进行进一步处理，以便提取更高级别的特征。最后，残差连接被应用于多头自注意力层和前馈全连接层，使得模型更容易训练和优化。\n",
    "\n",
    "由于TransformerEncoderLayer可以被堆叠起来形成多层编码器，因此它是自然语言处理和其他序列到序列任务中最常用的模型之一，比如翻译、文本分类等。\n",
    "\n",
    "\n",
    "**定义**：  \n",
    "torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=<function relu>, layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None)\n",
    "\n",
    "**参数**:  \n",
    "- d_model (int) – the number of expected features in the input (required).  输入中特征维度(必需的)。\n",
    "\n",
    "- nhead (int) – the number of heads in the multiheadattention models (required).  多头注意力模型中的头数(必需的)。\n",
    "\n",
    "- dim_feedforward (int) – the dimension of the feedforward network model (default=2048).  feedforward模块中Linear的维度\n",
    "\n",
    "- dropout (float) – the dropout value (default=0.1).  dropout值(默认=0.1)。\n",
    "\n",
    "- activation (Union[str, Callable[[Tensor], Tensor]]) – the activation function of the intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. Default: relu  中间层的激活函数，可以是一个字符串(\" relu \"或\" gelu \")或一个一元的可调用对象。默认值:relu\n",
    "\n",
    "- layer_norm_eps (float) – the eps value in layer normalization components (default=1e-5).  层归一化组件中的eps值(默认=1e-5)。\n",
    "\n",
    "- batch_first (bool) – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False (seq, batch, feature).  如果为True，则输入和输出张量将作为(batch, seq, feature)提供。默认值:False (seq, batch, feature)。\n",
    "\n",
    "- norm_first (bool) – if True, layer norm is done prior to attention and feedforward operations, respectively. Otherwise it’s done after. Default: False (after).  如果为True，则层norm分别在注意和前馈操作之前完成。否则以后再做。默认值:False(之后)。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39ed56e0",
   "metadata": {},
   "source": [
    "# 图解TransformerEncoderLayer\n",
    "Transformer的结构如下：\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/transformer.jpg\"\n",
    "    width=\"700\" /></p>\n",
    "左边的方块就是TransformerEncoderLayer，另外对TransformerEncoderLayer进行堆叠（N×），可以得到TransformerEncoder。\n",
    "\n",
    "下面对TransformerEncoderLayer的内部进行详细描述：\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/TransformerEncoderLayer.svg\"\n",
    "    width=\"2000\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d011221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:\n",
      " tensor([[[-0.1235,  1.6189, -1.0983, -0.3970]],\n",
      "\n",
      "        [[-1.5583,  1.2149,  0.0358,  0.3076]],\n",
      "\n",
      "        [[ 1.4580,  0.3129, -0.5627, -1.2082]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 单个样本简单举例\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(666)\n",
    "d_model = 4\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=2,dim_feedforward=2048)\n",
    "src = torch.rand(3, 1, d_model)  # L, N, E\n",
    "out = encoder_layer(src)\n",
    "print(\"out:\\n\", out, \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e00c79739f2fdf113306667eb0b8e68d4274855301e6df90bc305a954991b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
