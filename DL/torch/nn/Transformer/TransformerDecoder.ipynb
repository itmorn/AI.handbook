{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c202a11",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/itmorn/AI.handbook/blob/main/DL/torch/nn/Transformer/TransformerDecoder.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14a8662b",
   "metadata": {},
   "source": [
    "# TransformerDecoder\n",
    "TransformerDecoder是N个TransformerDecoderLayer的堆叠. \n",
    "\n",
    "**定义**：  \n",
    "torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)\n",
    "\n",
    "**参数**:  \n",
    "- decoder_layer – an instance of the TransformerDecoderLayer() class (required).  需要一个TransformerDecoderLayer对象\n",
    "\n",
    "- num_layers – the number of sub-decoder-layers in the decoder  (required).  堆叠多少层\n",
    "\n",
    "- norm – the layer normalization component (optional).   对最后的输出做norm的方式，默认是无"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39ed56e0",
   "metadata": {},
   "source": [
    "# 图解TransformerDecoder\n",
    "右边的方块×N就是TransformerDecoder，N就是num_layers\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/transformer.jpg\"\n",
    "    width=\"700\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d011221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:\n",
      " tensor([[[ 1.5059,  0.2510, -0.6276, -1.1293]],\n",
      "\n",
      "        [[-0.5577,  1.1509, -1.3426,  0.7494]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 单个样本简单举例\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(6688)\n",
    "d_model = 4\n",
    "\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=2, dim_feedforward=5, dropout=0.0)\n",
    "\n",
    "memory = torch.tensor([[[-0.1235, 1.6189, -1.0983, -0.3970]],\n",
    "                       [[-1.5583, 1.2149, 0.0358, 0.3076]],\n",
    "                       [[1.4580, 0.3129, -0.5627, -1.2082]]]) # TransformerEncoder的输出L_encoder_out, N, E\n",
    "\n",
    "# TransformerEncoder的输出L_encoder_out和TransformerDecoder的输入L_decoder_in可以是不同的。\n",
    "\n",
    "tgt = torch.rand(2, 1, d_model)  # TransformerDecoder的输入L_decoder_in, N, E\n",
    "out = decoder_layer(tgt, memory)\n",
    "print(\"out:\\n\", out, \"\\n\")  # 可以看到和上图输出的结果是一致的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f8ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:\n",
      " tensor([[[ 1.5059,  0.2510, -0.6276, -1.1293]],\n",
      "\n",
      "        [[-0.5577,  1.1509, -1.3426,  0.7494]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 单个样本简单举例\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(6688)\n",
    "d_model = 4\n",
    "\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=2, dim_feedforward=5, dropout=0.0)\n",
    "transformer_decoder  = torch.nn.TransformerDecoder(decoder_layer, num_layers=1, norm=None)\n",
    "\n",
    "memory = torch.tensor([[[-0.1235, 1.6189, -1.0983, -0.3970]],\n",
    "                       [[-1.5583, 1.2149, 0.0358, 0.3076]],\n",
    "                       [[1.4580, 0.3129, -0.5627, -1.2082]]]) # TransformerEncoder的输出L_encoder_out, N, E\n",
    "\n",
    "# TransformerEncoder的输出L_encoder_out和TransformerDecoder的输入L_decoder_in可以是不同的。\n",
    "\n",
    "tgt = torch.rand(2, 1, d_model)  # TransformerDecoder的输入L_decoder_in, N, E\n",
    "out = transformer_decoder(tgt, memory)\n",
    "print(\"out:\\n\", out, \"\\n\")  \n",
    "# 可以看到，当TransformerDecoder的num_layers=1时，结果和TransformerDecoderLayer是一致的。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e00c79739f2fdf113306667eb0b8e68d4274855301e6df90bc305a954991b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
