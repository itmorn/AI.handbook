{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c202a11",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/itmorn/AI.handbook/blob/main/DL/torch/nn/Transformer/Transformer.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14a8662b",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "Transformer是一种深度学习神经网络架构，最初由Vaswani等人在2017年提出，被广泛用于自然语言处理和其他序列到序列的任务，如机器翻译和语音识别。\n",
    "\n",
    "传统的序列到序列模型如RNN和LSTM等，通常需要在每一时刻对序列进行迭代，这会导致计算效率低下和难以并行化。而Transformer采用了注意力机制（Self-Attention）来计算序列中不同位置的相关性，从而可以同时处理整个序列，并且可以高效地并行化计算。\n",
    "\n",
    "Transformer的核心组件包括编码器（Encoder）和解码器（Decoder），分别用于将输入序列转换为抽象的表示和从该表示中生成输出序列。其中，编码器和解码器均由多个相同的层（Transformer Layer）组成，每个层都包含多头自注意力（Multi-Head Attention）和前馈神经网络（Feed-Forward Network）两个子层。多头自注意力可以学习序列中不同位置之间的关系，而前馈神经网络可以对这些关系进行变换和处理。\n",
    "\n",
    "Transformer在自然语言处理领域取得了很大的成功，例如Google的翻译服务就使用了Transformer模型。此外，Transformer也可以用于图像生成、音乐生成和语音识别等其他领域的任务。\n",
    "\n",
    "**定义**：  \n",
    "torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation=<function relu>, custom_encoder=None, custom_decoder=None, layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None)\n",
    "\n",
    "**参数**:  \n",
    "- d_model (int) – the number of expected features in the encoder/decoder inputs (default=512).  编码器/解码器输入的维度\n",
    "\n",
    "- nhead (int) – the number of heads in the multiheadattention models (default=8).  多头注意力模型中的头数\n",
    "\n",
    "- num_encoder_layers (int) – the number of sub-encoder-layers in the encoder (default=6).  编码器中的子编码器层的数量\n",
    "\n",
    "- num_decoder_layers (int) – the number of sub-decoder-layers in the decoder (default=6).  编码器中的子解码器层的数量\n",
    "\n",
    "- dim_feedforward (int) – the dimension of the feedforward network model (default=2048).  前馈网络模型的维度\n",
    "\n",
    "- dropout (float) – the dropout value (default=0.1).    dropout值\n",
    "\n",
    "- activation (Union[str, Callable[[Tensor], Tensor]]) – the activation function of encoder/decoder intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. Default: relu  编码器/解码器中间层的激活函数\n",
    "\n",
    "- custom_encoder (Optional[Any]) – custom encoder (default=None).  自定义编码器\n",
    "\n",
    "- custom_decoder (Optional[Any]) – custom decoder (default=None).  自定义解码器\n",
    "\n",
    "- layer_norm_eps (float) – the eps value in layer normalization components (default=1e-5).  层归一化组件中的eps值\n",
    "\n",
    "- batch_first (bool) – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False (seq, batch, feature).\n",
    "\n",
    "- norm_first (bool) – if True, encoder and decoder layers will perform LayerNorms before other attention and feedforward operations, otherwise after. Default: False (after)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39ed56e0",
   "metadata": {},
   "source": [
    "# 图解Transformer\n",
    "右边的方块×N就是TransformerDecoder，N就是num_layers\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/transformer.jpg\"\n",
    "    width=\"700\" /></p>\n",
    "\n",
    "pytorch实现的Transformer只覆盖下面的部分：\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/Transformer.svg\"\n",
    "    width=\"700\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d011221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:\n",
      " tensor([[[ 0.9859, -1.5983,  0.6843, -0.0720]],\n",
      "\n",
      "        [[-1.5189, -0.0553,  0.3122,  1.2619]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(666)\n",
    "d_model = 4\n",
    "\n",
    "src = torch.tensor([[[0.2180, 0.6484, 0.0964, 0.0614]],\n",
    "                    [[0.3732, 0.9106, 0.7293, 0.6012]],\n",
    "                    [[0.8778, 0.5302, 0.5404, 0.2252]]])  # TransformerEncoder的输入 L_encoder_in=3, N=1, E=4\n",
    "\n",
    "tgt = torch.tensor([[[0.6162, 0.1027, 0.0460, 0.0455]],\n",
    "                    [[0.2095, 0.7559, 0.2492, 0.9072]]])  # TransformerDecoder的输入  L_decoder_in=2, N=1, E=4\n",
    "\n",
    "transformer_model = nn.Transformer(d_model=d_model, nhead=2, num_encoder_layers=1, num_decoder_layers=1,\n",
    "                                   dim_feedforward=5, dropout=0.0)\n",
    "out = transformer_model(src, tgt)\n",
    "print(\"out:\\n\", out, \"\\n\")  # 可以看到和上图输出的结果是一致的\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e00c79739f2fdf113306667eb0b8e68d4274855301e6df90bc305a954991b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
