{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c202a11",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/itmorn/AI.handbook/blob/main/DL/torch/nn/LossFunction/CrossEntropyLoss.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14a8662b",
   "metadata": {},
   "source": [
    "# CrossEntropyLoss\n",
    "计算input logits和目标之间的交叉熵损失。\n",
    "\n",
    "以单个样本简单来说，CrossEntropyLoss的input是长度为num_classes的向量，先对input做Softmax转换，得到logits，再计算损失 $-\\sum{target*logits}$ 。\n",
    "\n",
    "如果是多个样本，我们知道，每个样本可以算出一个自己的loss，然后对多个样本进行reduce便可得到一个数值，reduce的方式可以通过reduction参数指定。  \n",
    "\n",
    "如果是类别不平衡的时候，可以通过weight对损失做加权。\n",
    "\n",
    "如果想做标签平滑可以使用label_smoothing参数。\n",
    "\n",
    "**定义**：  \n",
    "torch.nn.CrossEntropyLoss(weight=None, ignore_index=-100, reduction='mean', label_smoothing=0.0)\n",
    "\n",
    "**参数**:  \n",
    "- weight (Tensor, optional) – a manual rescaling weight given to each class. If given, has to be a Tensor of size C  给每个类一个手动缩放的权重。如果给定，必须是一个大小为C的张量\n",
    "\n",
    "- ignore_index (int, optional) – Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. Note that ignore_index is only applicable when the target contains class indices.  指定一个目标值，该目标值被忽略，不影响输入梯度。\n",
    "\n",
    "- reduction (str, optional) – Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.   指定应用于输出的缩减:'none' | 'mean' | 'sum'。\n",
    "\n",
    "- label_smoothing (float, optional) – A float in [0.0, 1.0]. Specifies the amount of smoothing when computing the loss, where 0.0 means no smoothing. The targets become a mixture of the original ground truth and a uniform distribution as described in Rethinking the Inception Architecture for Computer Vision. Default: 0.0.  标签平滑\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d357f94a",
   "metadata": {},
   "source": [
    "# 图解CrossEntropyLoss\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/CrossEntropyLoss.svg\"\n",
    "    width=\"2000\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d011221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " tensor([[-2.1188,  0.0635, -1.4555, -0.0126, -0.1548]], requires_grad=True) \n",
      "\n",
      "target:\n",
      " tensor([1]) \n",
      "\n",
      "output:\n",
      " tensor(1.1192, grad_fn=<NllLossBackward0>) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.1192, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 单个样本简单举例\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "N = 1\n",
    "C = 5 #num_classes\n",
    "input = torch.randn(N, C, requires_grad=True)\n",
    "\n",
    "target = torch.empty(N, dtype=torch.long).random_(5)  # 支持target是类别索引的方式\n",
    "# target = torch.tensor([[0.0, 1.0, 0.0, 0.0, 0.0]])  # 也支持target是onehot的方式\n",
    "print(\"input:\\n\", input, \"\\n\")\n",
    "print(\"target:\\n\", target, \"\\n\")\n",
    "\n",
    "output = loss(input, target)\n",
    "print(\"output:\\n\", output, \"\\n\")\n",
    "output.backward()\n",
    "\n",
    "-torch.log(input.softmax(dim=1)[0][1])  # 可以看到，和调包结果一致\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "41a42d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " tensor([[-2.1188,  0.0635, -1.4555, -0.0126, -0.1548]], requires_grad=True) \n",
      "\n",
      "output:\n",
      " tensor(1.5187, grad_fn=<AddBackward0>) \n",
      "\n",
      "new_onehot_labels:\n",
      " tensor([0.1000, 0.6000, 0.1000, 0.1000, 0.1000]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.5187, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 演示label_smoothing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "label_smoothing = 0.5\n",
    "N = 1\n",
    "C = 5\n",
    "input = torch.randn(N, C, requires_grad=True)\n",
    "onehot_target = torch.tensor([0, 1, 0, 0, 0])\n",
    "print(\"input:\\n\", input, \"\\n\")\n",
    "# print(\"onehot_target:\\n\", onehot_target, \"\\n\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction='sum', label_smoothing=label_smoothing)\n",
    "# new_onehot_labels = onehot_labels * (1 - label_smoothing) + label_smoothing / num_classes\n",
    "# 值为1的元素减去label_smoothing，然后把label_smoothing再平分给每个元素\n",
    "\n",
    "output = loss(input, target)\n",
    "print(\"output:\\n\", output, \"\\n\")\n",
    "output.backward()\n",
    "\n",
    "new_onehot_labels = onehot_target * (1 - label_smoothing) + label_smoothing / C\n",
    "# print(\"weight:\\n\", weight, \"\\n\")\n",
    "print(\"new_onehot_labels:\\n\", new_onehot_labels, \"\\n\")\n",
    "\n",
    "-(input.softmax(dim=1)[0].log()*new_onehot_labels).sum()  # 可以看到，和调包结果一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "527246e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " tensor([[-2.1188,  0.0635, -1.4555, -0.0126, -0.1548]], requires_grad=True) \n",
      "\n",
      "output:\n",
      " tensor(3.3575, grad_fn=<NllLossBackward0>) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.3575, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 演示weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "N = 1\n",
    "C = 5\n",
    "input = torch.randn(N, C, requires_grad=True)\n",
    "weight = torch.tensor([4.0, 3.0, 2.0, 1.0, 1.0])\n",
    "onehot_target = torch.tensor([0, 1, 0, 0, 0])\n",
    "print(\"input:\\n\", input, \"\\n\")\n",
    "# print(\"onehot_target:\\n\", onehot_target, \"\\n\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss(weight=weight, reduction='sum', label_smoothing=0)\n",
    "# new_onehot_labels = onehot_labels * (1 - label_smoothing) + label_smoothing / num_classes\n",
    "# 值为1的元素减去label_smoothing，然后把label_smoothing再平分给每个元素\n",
    "\n",
    "\n",
    "output = loss(input, target)\n",
    "print(\"output:\\n\", output, \"\\n\")\n",
    "output.backward()\n",
    "\n",
    "\n",
    "-(input.softmax(dim=1)[0].log()*onehot_target*weight).sum()  # 可以看到，和调包结果一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "53fab57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " tensor([[-2.1188,  0.0635, -1.4555, -0.0126, -0.1548]], requires_grad=True) \n",
      "\n",
      "weight:\n",
      " tensor([3., 2., 2., 1., 1.]) \n",
      "\n",
      "output:\n",
      " tensor(3.1143, grad_fn=<AddBackward0>) \n",
      "\n",
      "new_onehot_labels:\n",
      " tensor([0.1000, 0.6000, 0.1000, 0.1000, 0.1000]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.1143, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 演示label_smoothing + weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "label_smoothing = 0.5\n",
    "N = 1\n",
    "C = 5\n",
    "input = torch.randn(N, C, requires_grad=True)\n",
    "weight = torch.tensor([3.0, 2.0, 2.0, 1.0, 1.0])\n",
    "onehot_target = torch.tensor([0, 1, 0, 0, 0])\n",
    "print(\"input:\\n\", input, \"\\n\")\n",
    "print(\"weight:\\n\", weight, \"\\n\")\n",
    "\n",
    "\n",
    "loss = nn.CrossEntropyLoss(\n",
    "    weight=weight, reduction='sum', label_smoothing=label_smoothing)\n",
    "# new_onehot_labels = onehot_labels * (1 - label_smoothing) + label_smoothing / num_classes\n",
    "# 值为1的元素减去label_smoothing，然后把label_smoothing再平分给每个元素\n",
    "\n",
    "output = loss(input, target)\n",
    "print(\"output:\\n\", output, \"\\n\")\n",
    "output.backward()\n",
    "\n",
    "new_onehot_labels = onehot_target * (1 - label_smoothing) + label_smoothing / C\n",
    "# print(\"weight:\\n\", weight, \"\\n\")\n",
    "print(\"new_onehot_labels:\\n\", new_onehot_labels, \"\\n\")\n",
    "\n",
    "-(input.softmax(dim=1)[0].log()*new_onehot_labels*weight).sum()  # 可以看到，和调包结果一致"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e00c79739f2fdf113306667eb0b8e68d4274855301e6df90bc305a954991b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
