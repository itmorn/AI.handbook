{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c202a11",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/itmorn/AI.handbook/blob/main/DL/torch/nn/LossFunction/KLDivLoss.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14a8662b",
   "metadata": {},
   "source": [
    "# KLDivLoss\n",
    "Kullback-Leibler散度损失。\n",
    "$$D_{KL}(p||q)=\\sum_{i=1}^{n}{p(x_i)log(\\frac{p(x_i)}{q(x_i)})}$$\n",
    "在机器学习中，P往往用来表示样本的真实分布，Q用来表示模型所预测的分布，那么KL散度就可以计算两个分布的差异，也就是Loss损失值。\n",
    "\n",
    "**定义**：  \n",
    "torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)\n",
    "\n",
    "**参数**:  \n",
    "- reduction (str, optional) – Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.   指定应用于输出的reduce方式:'none' | 'mean' | 'sum'。\n",
    "\n",
    "- log_target (bool, optional) – Specifies whether target is the log space. Default: False  target是否为log空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d011221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " tensor([[-3.3015, -1.1192, -2.6382, -1.1952, -1.3375],\n",
      "        [-2.9087, -0.2243, -2.3617, -3.5050, -3.8121],\n",
      "        [-1.7003, -1.7383, -1.0231, -1.6480, -2.4119]],\n",
      "       grad_fn=<LogSoftmaxBackward0>) \n",
      "\n",
      "target:\n",
      " tensor([[0.1605, 0.1574, 0.2878, 0.2175, 0.1767],\n",
      "        [0.1900, 0.1799, 0.1236, 0.3104, 0.1961],\n",
      "        [0.1795, 0.1527, 0.1553, 0.2535, 0.2590]]) \n",
      "\n",
      "loss:\n",
      " tensor(0.5752, grad_fn=<DivBackward0>) \n",
      "\n",
      "tensor(0.5752, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "log_target = False\n",
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\", log_target=log_target)\n",
    "# input should be a distribution in the log space 注意input一定是log space的\n",
    "input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n",
    "print(\"input:\\n\", input, \"\\n\")\n",
    "\n",
    "# Sample a batch of distributions. Usually this would come from the dataset\n",
    "target = F.softmax(torch.rand(3, 5), dim=1)\n",
    "print(\"target:\\n\", target, \"\\n\")\n",
    "\n",
    "loss = kl_loss(input, target)\n",
    "print(\"loss:\\n\", loss, \"\\n\")\n",
    "\n",
    "if (log_target):\n",
    "    print((target.exp()*(target - input)).sum(dim=1).mean())\n",
    "else:\n",
    "    print((torch.xlogy(target, target) - target * input).sum(dim=1).mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e00c79739f2fdf113306667eb0b8e68d4274855301e6df90bc305a954991b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
