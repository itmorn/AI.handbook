{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36e625b5",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/itmorn/AI.handbook/blob/main/DL/torch/nn/Recurrent/GRUCell.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcf66449",
   "metadata": {},
   "source": [
    "# GRUCell  VS  LSTMCell\n",
    "LSTM的一个稍微更具戏剧性的变化是Cho等人(2014)引入的门控循环单元(gate Recurrent Unit, GRU)。它将遗忘门和输入门合并为一个“更新门”。它还合并了记忆单元状态（cell state）和隐藏状态（hidden state），并进行了一些其他更改。由此产生的模型比标准LSTM模型更简单，并且越来越受欢迎。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14a8662b",
   "metadata": {},
   "source": [
    "# GRUCell\n",
    "\n",
    "**定义**：   \n",
    "torch.nn.GRUCell(input_size, hidden_size, bias=True, device=None, dtype=None)\n",
    "\n",
    "**参数**：  \n",
    "- input_size (int) – The number of expected features in the input x.  时间序列某一时刻的特征向量长度\n",
    "\n",
    "- hidden_size (int) – The number of features in the hidden state h.  隐藏层向量长度\n",
    "\n",
    "- bias (bool) – If False, then the layer does not use bias weights b_ih and b_hh. Default: True.  是否加待学习的偏置项\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38fd4b85",
   "metadata": {},
   "source": [
    "# 图解前向过程\n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/LSTM2-notation.png\"\n",
    "    width=\"700\" /></p>\n",
    "    \n",
    "<p align=\"center\">\n",
    "<img src=\"./imgs/LSTM3-var-GRU.png\"\n",
    "    width=\"1000\" /></p>\n",
    "\n",
    "由于整个过程和LSTM是一致的，所以就不做手工验证以及绘图了，这里只展示调包的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8a5a4e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " tensor([[[-2.1188,  0.0635, -1.4555]],\n",
      "\n",
      "        [[-0.0126, -0.1548, -0.0927]]]) \n",
      "\n",
      "h_0:\n",
      " tensor([[ 2.5916,  0.4542, -0.6890, -0.9962]]) \n",
      "\n",
      "weight_hh:\n",
      " Parameter containing:\n",
      "tensor([[-0.0440,  0.2744, -0.1232,  0.2019],\n",
      "        [ 0.0582,  0.3487, -0.1404,  0.3816],\n",
      "        [ 0.1317,  0.1924, -0.3796,  0.4379],\n",
      "        [ 0.1783,  0.1848, -0.2799, -0.0428],\n",
      "        [ 0.1167, -0.0991, -0.2574, -0.4477],\n",
      "        [-0.0506, -0.1730, -0.3312,  0.0733],\n",
      "        [-0.1884, -0.2347,  0.1158,  0.3620],\n",
      "        [-0.1595,  0.2099, -0.4129,  0.0649],\n",
      "        [ 0.4904, -0.2916, -0.2753, -0.2733],\n",
      "        [ 0.1248,  0.1446, -0.4906, -0.3950],\n",
      "        [-0.4422,  0.3924, -0.4710, -0.3778],\n",
      "        [ 0.2299,  0.0562, -0.3475,  0.2820]], requires_grad=True) \n",
      "\n",
      "weight_ih:\n",
      " Parameter containing:\n",
      "tensor([[-0.2366, -0.1316,  0.0035],\n",
      "        [ 0.4089, -0.1196,  0.2539],\n",
      "        [-0.0837, -0.4911, -0.4336],\n",
      "        [ 0.3111, -0.2333, -0.4399],\n",
      "        [-0.1921, -0.2115,  0.3916],\n",
      "        [ 0.1119, -0.0959, -0.0424],\n",
      "        [-0.0969, -0.4728,  0.4485],\n",
      "        [-0.0107, -0.3058, -0.4678],\n",
      "        [-0.4511,  0.0390,  0.0608],\n",
      "        [ 0.0176,  0.2500, -0.0070],\n",
      "        [-0.0432, -0.2586, -0.1025],\n",
      "        [-0.2100, -0.1622, -0.3667]], requires_grad=True) \n",
      "\n",
      "h_1:\n",
      " tensor([[ 2.0482,  0.3345, -0.1018, -0.3265]], grad_fn=<AddBackward0>) \n",
      "\n",
      "h_2:\n",
      " tensor([[ 1.4025,  0.2701, -0.2244,  0.0220]], grad_fn=<AddBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调包计算\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "L = 2  # sequence_length  也可理解为time_steps\n",
    "N = 1  # batch_size\n",
    "H_in = 3  # input_size 输入层特征向量的长度\n",
    "H_out = 4  # hidden_size 隐藏层向量的长度\n",
    "\n",
    "input = torch.randn(L, N, H_in) # (time_steps, batch, input_size)\n",
    "h = torch.randn(N, H_out) # (batch, hidden_size) 负责决定如何改变“记忆”\n",
    "print(\"input:\\n\", input, \"\\n\")\n",
    "print(\"h_0:\\n\", h, \"\\n\")\n",
    "\n",
    "gru_cell = nn.GRUCell(H_in, H_out, bias=False) # (input_size, hidden_size) 为了画图简洁，不要偏置项\n",
    "print(\"weight_hh:\\n\", gru_cell.weight_hh, \"\\n\")\n",
    "print(\"weight_ih:\\n\", gru_cell.weight_ih, \"\\n\")\n",
    "\n",
    "output = []  #保存每个时刻的隐藏层的数据\n",
    "for i in range(input.size()[0]):\n",
    "    h = gru_cell(input[i], h)\n",
    "    print(f\"h_{i+1}:\\n\", h, \"\\n\")\n",
    "    output.append(h)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "577d3767",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e00c79739f2fdf113306667eb0b8e68d4274855301e6df90bc305a954991b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
