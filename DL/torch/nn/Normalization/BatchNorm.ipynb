{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c202a11",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/itmorn/AI.handbook/blob/main/DL/torch/nn/Normalization/BatchNorm.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5574623c",
   "metadata": {},
   "source": [
    "# 简介批量归一化\n",
    "归一化的思想比较早，而在卷积网络中提出BatchNorm层是发生在2016年。\n",
    "\n",
    "在训练阶段，我们会对每一个批次，在通道维度上求均值EX和样本方差VarX，然后对该批次数据在通道维度上进行规范化；另外BN层还有两个可以学习的参数（ $γ$ 和 $β$ ），它们可以对数据的分布进行二次矫正。再另外，训练阶段会使用移动平均的方式近似维护训练数据整体的均值（running_mean）和整体的无偏方差（running_var），以供推理时对单个样本进行分布矫正。\n",
    "\n",
    "在推理阶段，直接使用整体的均值（running_mean）和整体的无偏方差（running_var）对数据进行规范化，再使用 $γ$ 和 $β$ 进行二次矫正。\n",
    "\n",
    "\n",
    "批量归一化可以加速收敛速度，但一般不改变模型精度。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb6244b6",
   "metadata": {},
   "source": [
    "# 为什么要用BatchNorm\n",
    "当神经网络深度比较深的时候，会出现梯度消失的问题，即靠近loss的层参数更新的快，靠近input的层参数更新的慢，这主要是由误差反向传播时，数据的分布不规范，可能很多数据都落在激活函数导函数比较小的位置，最终累乘的结果比较小导致的。而BN层就可以对每一层的数据分布进行矫正，从而达到稳定训练，快速收敛的效果。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e568207",
   "metadata": {},
   "source": [
    "# 为什么BatchNorm不与dropout同时使用\n",
    "由于训练时，每一个小批量都要计算均值和方差，然后对每个样本进行规范化。这就相当于再给数据加噪声，已经可以起到控制模型复杂度的效果了。就没必要再使用dropout。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14a8662b",
   "metadata": {},
   "source": [
    "# BatchNorm2d\n",
    "在4D输入上应用批归一化(NCHW)，论文参考[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "**定义**：  \n",
    "torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)\n",
    "\n",
    "**公式**：  \n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "均值和标准偏差在小批量上按维度计算， $γ$ 和 $β$ 是大小为 $C$ 的可学习参数向量(其中 $C$ 为NCHW中的C)。默认情况下， $γ$ 的元素被设置为1， $β$ 的元素被设置为0。标准偏差通过**有偏估计**来计算，相当于torch.var(input, unbiased=False)。\n",
    "\n",
    "同样在默认情况下，在训练期间，该层保存对其计算出的平均值和方差的估计，然后在评估期间用于归一化。运行估计保持默认动量为0.1。如果track_running_stats设置为False，则该层不会继续运行全局均值和全局无偏方差的估计，而是在评估期间使用批量统计。\n",
    "\n",
    "**参数**:  \n",
    "- num_features (int) – $C$ from an expected input of size $(N, C, H, W)$.  $C$ 是输入张量尺寸$(N, C, H, W)$中的 $C$ 。\n",
    "\n",
    "- eps (float) – a value added to the denominator for numerical stability. Default: 1e-5.  为数值稳定性添加到分母的值。默认值:1e-5\n",
    "\n",
    "- momentum (float) – the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1.  用于running_mean（全局均值）和running_var（全局无偏方差）计算的值。累积移动平均，可以设置为None。默认值:0.1。\n",
    "\n",
    "- affine (bool) – a boolean value that when set to True, this module has learnable affine parameters. Default: True.  一个布尔值，当设置为True时，该模块具有可学习的仿射参数(也就是 $γ$ 和 $β$ )。默认值:True\n",
    "\n",
    "- track_running_stats (bool) – a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as None. When these buffers are None, this module always uses batch statistics in both training and eval modes. Default: True.  当设置为True时，此模块跟踪运行的平均值和方差；当设置为False时，此模块不跟踪此类统计，并将统计缓冲区running_mean和running_var初始化为None。当这些缓冲区为None时，该模块在训练和评估模式中总是使用批量统计。默认值:True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c679ddde",
   "metadata": {},
   "source": [
    "# 图解train模式下的前向传播过程\n",
    "\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/torch/nn/Normalization/imgs/BatchNorm.svg\">\n",
    "<img src=\"./imgs/BatchNorm.svg\"\n",
    "    width=\"2000\" /></a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "41c3221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input1:\n",
      " tensor([[[[ 1.,  6.],\n",
      "          [ 9.,  4.]],\n",
      "\n",
      "         [[12., 18.],\n",
      "          [13., 11.]]],\n",
      "\n",
      "\n",
      "        [[[ 2.,  7.],\n",
      "          [ 3.,  8.]],\n",
      "\n",
      "         [[19., 17.],\n",
      "          [15., 11.]]]]) \n",
      "\n",
      "Ex:\n",
      " tensor([[[[ 5.0000]],\n",
      "\n",
      "         [[14.5000]]]]) \n",
      "\n",
      "VarX:\n",
      " tensor([[[[7.5000]],\n",
      "\n",
      "         [[9.0000]]]]) \n",
      "\n",
      "input1-Ex:\n",
      " tensor([[[[-4.0000,  1.0000],\n",
      "          [ 4.0000, -1.0000]],\n",
      "\n",
      "         [[-2.5000,  3.5000],\n",
      "          [-1.5000, -3.5000]]],\n",
      "\n",
      "\n",
      "        [[[-3.0000,  2.0000],\n",
      "          [-2.0000,  3.0000]],\n",
      "\n",
      "         [[ 4.5000,  2.5000],\n",
      "          [ 0.5000, -3.5000]]]]) \n",
      "\n",
      "sqrt(VarX+eps):\n",
      " tensor([[[[2.7386]],\n",
      "\n",
      "         [[3.0000]]]]) \n",
      "\n",
      "(input1-Ex)/sqrt(VarX+eps):\n",
      " tensor([[[[-1.4606,  0.3651],\n",
      "          [ 1.4606, -0.3651]],\n",
      "\n",
      "         [[-0.8333,  1.1667],\n",
      "          [-0.5000, -1.1667]]],\n",
      "\n",
      "\n",
      "        [[[-1.0954,  0.7303],\n",
      "          [-0.7303,  1.0954]],\n",
      "\n",
      "         [[ 1.5000,  0.8333],\n",
      "          [ 0.1667, -1.1667]]]]) \n",
      "\n",
      "[(input1-Ex)/sqrt(VarX+eps)] * γ + β:\n",
      " tensor([[[[-1.4606,  0.3651],\n",
      "          [ 1.4606, -0.3651]],\n",
      "\n",
      "         [[-0.8333,  1.1667],\n",
      "          [-0.5000, -1.1667]]],\n",
      "\n",
      "\n",
      "        [[[-1.0954,  0.7303],\n",
      "          [-0.7303,  1.0954]],\n",
      "\n",
      "         [[ 1.5000,  0.8333],\n",
      "          [ 0.1667, -1.1667]]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 手工计算\n",
    "import torch\n",
    "\n",
    "input1 = torch.tensor([\n",
    "    [\n",
    "        [[1, 6],\n",
    "         [9, 4]],\n",
    "        [[12, 18],\n",
    "         [13, 11]]],\n",
    "    [\n",
    "        [[2, 7],\n",
    "         [3, 8]],\n",
    "        [[19, 17],\n",
    "         [15, 11]]\n",
    "    ]\n",
    "], dtype=torch.float32)\n",
    "print(\"input1:\\n\", input1, \"\\n\")\n",
    "\n",
    "# 第1步：按照通道求均值和方差：\n",
    "VarX, EX = torch.var_mean(input, dim=(0, 2, 3), keepdim=True, unbiased=False)  # NCHW\n",
    "print(\"Ex:\\n\", EX, \"\\n\")\n",
    "print(\"VarX:\\n\", VarX, \"\\n\")\n",
    "\n",
    "# 第2步：减去均值：\n",
    "result2 = input1-EX\n",
    "print(\"input1-Ex:\\n\", result2, \"\\n\")\n",
    "\n",
    "# 第3步：求sqrt(VarX+eps)：\n",
    "eps = 1e-5\n",
    "result3 = torch.sqrt(VarX+eps)\n",
    "print(\"sqrt(VarX+eps):\\n\", result3, \"\\n\")\n",
    "\n",
    "# 第4步：第2步的结果/第3步的结果，完成batch内的数据规范化:\n",
    "result4 = result2/result3\n",
    "print(\"(input1-Ex)/sqrt(VarX+eps):\\n\", result4, \"\\n\")\n",
    "\n",
    "# 第5步：使用γ=1，β=0 进行再校正：\n",
    "γ=1\n",
    "β=0\n",
    "result5 = result4 * γ + β\n",
    "print(\"[(input1-Ex)/sqrt(VarX+eps)] * γ + β:\\n\", result5, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d011221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input1:\n",
      " tensor([[[[ 1.,  6.],\n",
      "          [ 9.,  4.]],\n",
      "\n",
      "         [[12., 18.],\n",
      "          [13., 11.]]],\n",
      "\n",
      "\n",
      "        [[[ 2.,  7.],\n",
      "          [ 3.,  8.]],\n",
      "\n",
      "         [[19., 17.],\n",
      "          [15., 11.]]]]) \n",
      "\n",
      "nn.BatchNorm2d默认初始化可学习参数γ=1:\n",
      " Parameter containing:\n",
      "tensor([1., 1.], requires_grad=True) \n",
      "\n",
      "nn.BatchNorm2d默认初始化可学习参数β=0:\n",
      " Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True) \n",
      "\n",
      "output:\n",
      " tensor([[[[-1.4606,  0.3651],\n",
      "          [ 1.4606, -0.3651]],\n",
      "\n",
      "         [[-0.8333,  1.1667],\n",
      "          [-0.5000, -1.1667]]],\n",
      "\n",
      "\n",
      "        [[[-1.0954,  0.7303],\n",
      "          [-0.7303,  1.0954]],\n",
      "\n",
      "         [[ 1.5000,  0.8333],\n",
      "          [ 0.1667, -1.1667]]]], grad_fn=<NativeBatchNormBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调包计算\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input1 = torch.tensor([\n",
    "    [\n",
    "        [[1, 6],\n",
    "         [9, 4]],\n",
    "        [[12, 18],\n",
    "         [13, 11]]],\n",
    "    [\n",
    "        [[2, 7],\n",
    "         [3, 8]],\n",
    "        [[19, 17],\n",
    "         [15, 11]]\n",
    "    ]\n",
    "], dtype=torch.float32)\n",
    "print(\"input1:\\n\", input1,\"\\n\")\n",
    "\n",
    "m = nn.BatchNorm2d(num_features=2, eps=1e-5, momentum=1, affine=True, track_running_stats=True)\n",
    "m.train()\n",
    "\n",
    "print(\"nn.BatchNorm2d默认初始化可学习参数γ=1:\\n\", m.weight,\"\\n\")\n",
    "print(\"nn.BatchNorm2d默认初始化可学习参数β=0:\\n\", m.bias,\"\\n\")\n",
    "\n",
    "output = m(input1)\n",
    "print(\"output:\\n\", output,\"\\n\") # 结果和手工计算一致"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c484e70",
   "metadata": {},
   "source": [
    "# 图解train模式下维护全局均值方差\n",
    "我们知道，在模型的train过程中，每次前向传播会处理一个batch的样本，此时，我们可以在这个小批次上统计每个通道的均值和方差，进而完成小批次内的规范化，即：\n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} $$\n",
    "\n",
    "然而，在推理的过程中，我们往往都是对一个样本进行推理，这时候就没办法进行规范化了。那有什么解决方案吗？当然。比如我们可以计算全部训练数据的均值和方差，供推理的时候使用。这个方法可行，且比较准确的反馈了数据整体的均值和无偏方差，但问题是计算成本比较大。torch中采用的是**移动平均**的计算方法。下面简述一下步骤：  \n",
    "\n",
    "1. 初始化：全局均值=0，全局方差=1。\n",
    "2. 计算：第 i 批数据计算出来的均值=EX_i，方差=VarX_i.\n",
    "3. 更新：全局均值 = EX_i * momentum + 全局均值 * (1-momentum).\n",
    "4. 更新：全局方差 = VarX_i * momentum + 全局方差 * (1-momentum).\n",
    "5. 重复2,3,4直到训练结束，便可得到**全局均值**和**全局方差**。\n",
    "\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/torch/nn/Normalization/imgs/BatchNorm2.svg\">\n",
    "<img src=\"./imgs/BatchNorm2.svg\"\n",
    "    width=\"2000\" /></a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "81cc3b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn.BatchNorm2d默认初始化全局均值:\n",
      " tensor([0., 0.]) \n",
      "\n",
      "nn.BatchNorm2d默认初始化全局方差:\n",
      " tensor([1., 1.]) \n",
      "\n",
      "\n",
      "\n",
      "nn.BatchNorm2d全局均值 after 1 batch:\n",
      " tensor([0.5000, 1.4500]) \n",
      "\n",
      "nn.BatchNorm2d全局方差 after 1 batch:\n",
      " tensor([1.7571, 1.9286]) \n",
      "\n",
      "\n",
      "\n",
      "nn.BatchNorm2d全局均值 after 2 batch:\n",
      " tensor([0.9500, 2.7550]) \n",
      "\n",
      "nn.BatchNorm2d全局方差 after 2 batch:\n",
      " tensor([2.4386, 2.7643]) \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调包计算训练过程中BN维护的全局均值和方差\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input1 = torch.tensor([\n",
    "    [\n",
    "        [[1, 6],\n",
    "         [9, 4]],\n",
    "        [[12, 18],\n",
    "         [13, 11]]],\n",
    "    [\n",
    "        [[2, 7],\n",
    "         [3, 8]],\n",
    "        [[19, 17],\n",
    "         [15, 11]]\n",
    "    ]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "m = nn.BatchNorm2d(num_features=2, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True)\n",
    "m.train()\n",
    "\n",
    "print(\"nn.BatchNorm2d默认初始化全局均值:\\n\", m.running_mean,\"\\n\")\n",
    "print(\"nn.BatchNorm2d默认初始化全局方差:\\n\", m.running_var,\"\\n\\n\\n\")\n",
    "\n",
    "output = m(input1)\n",
    "\n",
    "print(\"nn.BatchNorm2d全局均值 after 1 batch:\\n\", m.running_mean,\"\\n\")\n",
    "print(\"nn.BatchNorm2d全局方差 after 1 batch:\\n\", m.running_var,\"\\n\\n\\n\")\n",
    "\n",
    "output = m(input1)\n",
    "\n",
    "print(\"nn.BatchNorm2d全局均值 after 2 batch:\\n\", m.running_mean,\"\\n\")\n",
    "print(\"nn.BatchNorm2d全局方差 after 2 batch:\\n\", m.running_var,\"\\n\\n\\n\") # 和上图中的结果一致"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a8c81f5",
   "metadata": {},
   "source": [
    "# eval模式下的前向传播\n",
    "推理过程会使用训练模式下维护的全局均值和全局无偏方差，且不会再改变它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ab242640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================train模式下训练2个batch后的全局均值和全局无偏方差=============\n",
      "nn.BatchNorm2d全局均值 after 2 batch:\n",
      " tensor([0.9500, 2.7550]) \n",
      "\n",
      "nn.BatchNorm2d全局方差 after 2 batch:\n",
      " tensor([2.4386, 2.7643])\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "\n",
      "===================eval模式下推理2个batch后的全局均值和全局无偏方差=============\n",
      "nn.BatchNorm2d全局均值 after 2 batch:\n",
      " tensor([0.9500, 2.7550]) \n",
      "\n",
      "nn.BatchNorm2d全局方差 after 2 batch:\n",
      " tensor([2.4386, 2.7643])\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "\n",
      "output:\n",
      " tensor([[[[0.0320, 3.2339],\n",
      "          [5.1550, 1.9531]],\n",
      "\n",
      "         [[5.5605, 9.1693],\n",
      "          [6.1620, 4.9590]]],\n",
      "\n",
      "\n",
      "        [[[0.6724, 3.8742],\n",
      "          [1.3128, 4.5146]],\n",
      "\n",
      "         [[9.7707, 8.5678],\n",
      "          [7.3649, 4.9590]]]], grad_fn=<NativeBatchNormBackward0>) \n",
      "\n",
      "result5:\n",
      " tensor([[[[ 0.0320,  1.9517],\n",
      "          [ 5.1550,  0.7488]],\n",
      "\n",
      "         [[ 7.0761,  9.1693],\n",
      "          [ 7.7165,  4.9590]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6724,  2.5532],\n",
      "          [ 1.3128,  3.1547]],\n",
      "\n",
      "         [[11.5587,  8.5678],\n",
      "          [ 8.9972,  4.9590]]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调包计算训练过程中BN维护的全局均值和方差\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input1 = torch.tensor([\n",
    "    [\n",
    "        [[1, 6],\n",
    "         [9, 4]],\n",
    "        [[12, 18],\n",
    "         [13, 11]]],\n",
    "    [\n",
    "        [[2, 7],\n",
    "         [3, 8]],\n",
    "        [[19, 17],\n",
    "         [15, 11]]\n",
    "    ]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "m = nn.BatchNorm2d(num_features=2, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "m.train()\n",
    "output = m(input1)\n",
    "output = m(input1)\n",
    "\n",
    "print(\"===================train模式下训练2个batch后的全局均值和全局无偏方差=============\")\n",
    "print(\"nn.BatchNorm2d全局均值 after 2 batch:\\n\", m.running_mean,\"\\n\")\n",
    "print(\"nn.BatchNorm2d全局方差 after 2 batch:\\n\", m.running_var)\n",
    "print(\"==============================================================================\\n\\n\\n\")\n",
    "\n",
    "\n",
    "m.eval()\n",
    "output = m(input1)\n",
    "output = m(input1)\n",
    "print(\"===================eval模式下推理2个batch后的全局均值和全局无偏方差=============\")\n",
    "print(\"nn.BatchNorm2d全局均值 after 2 batch:\\n\", m.running_mean,\"\\n\")\n",
    "print(\"nn.BatchNorm2d全局方差 after 2 batch:\\n\", m.running_var)\n",
    "print(\"==============================================================================\\n\\n\\n\")\n",
    "# 可以看到eval模式下全局均值和全局无偏方差不再改变。\n",
    "print(\"output:\\n\", output,\"\\n\") \n",
    "\n",
    "# 手工计算\n",
    "result2 = input1-m.running_mean\n",
    "eps = 1e-5\n",
    "result3 = torch.sqrt(m.running_var+eps)\n",
    "result4 = result2/result3\n",
    "γ=1\n",
    "β=0\n",
    "result5 = result4 * γ + β\n",
    "print(\"result5:\\n\", result5,\"\\n\")  #可以看到调包计算和手工计算的结果一致\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8844500",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "[图解深度学习与神经网络：从张量到TensorFlow实现》_张平](https://item.jd.com/12429187.html)\n",
    "[28 批量归一化【动手学深度学习v2】](https://www.bilibili.com/video/BV1X44y1r77r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e00c79739f2fdf113306667eb0b8e68d4274855301e6df90bc305a954991b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
