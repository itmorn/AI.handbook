{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e625b5",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/itmorn/AI.handbook/blob/main/DL/module/Convolution/Conv.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8662b",
   "metadata": {},
   "source": [
    "# Conv1d\n",
    "对由多个输入平面组成的输入信号应用1D卷积。\n",
    "\n",
    "定义：   \n",
    "torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "参数：  \n",
    "in_channels (int): Number of channels in the input image\n",
    "输入图像中的通道数\n",
    "\n",
    "out_channels (int): Number of channels produced by the convolution\n",
    "卷积产生的通道数\n",
    "\n",
    "kernel_size (int or tuple): Size of the convolving kernel\n",
    "卷积核的大小\n",
    "\n",
    "stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "卷积步幅。默认值:1\n",
    "\n",
    "padding (int, tuple or str, optional): Padding added to both sides of the input. Default: 0\n",
    "输入框两边的添加的Padding尺寸。默认值:0\n",
    "\n",
    "padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
    "添加Padding的方式\n",
    "\n",
    "dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "卷积核元素之间的间距\n",
    "\n",
    "groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
    "分组卷积\n",
    "\n",
    "bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
    "添加一个可学习的偏置项\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006d014",
   "metadata": {},
   "source": [
    "## 图解in_channels、out_channels、kernel_size\n",
    "\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/conv1d.svg\">\n",
    "<img src=\"./imgs/conv1d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05f9d3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[-0.7747,  0.7926, -0.0062, -0.4377,  0.7148],\n",
      "         [ 0.3590, -0.1242,  2.0345, -0.3479, -0.4007]],\n",
      "\n",
      "        [[ 0.8059, -0.1021,  0.3168, -0.8889,  1.1768],\n",
      "         [ 0.8074,  0.9144,  1.6259, -0.6535, -0.0865]]])\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "output: tensor([[[-0.1682,  0.7688, -0.9502],\n",
      "         [-0.0725,  0.0577, -0.0527]],\n",
      "\n",
      "        [[-0.0756,  0.3155, -0.9533],\n",
      "         [-0.1843,  0.3324, -0.1548]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2 # 一般用C_in或C代替\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "print(\"input:\",input)\n",
    "\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4680baca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.16810998000000002\n",
      "-0.07245857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_part = np.array([\n",
    "    [-0.7747, 0.7926, -0.0062],\n",
    "    [0.3590, -0.1242, 2.0345]\n",
    "])\n",
    "\n",
    "kernel_1 = np.array([\n",
    "    [-0.0783, -0.0347, -0.0791],\n",
    "    [-0.3860, 0.3662, -0.0087]\n",
    "])\n",
    "print(np.sum(input_part*kernel_1))\n",
    "\n",
    "kernel_2 = np.array([\n",
    "    [-0.2497, -0.3820, -0.3683],\n",
    "    [0.0319,  0.0496,  0.0144]\n",
    "])\n",
    "print(np.sum(input_part*kernel_2))\n",
    "# 结果和图上会有些许差异，主要是四舍五入所导致的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75607d01",
   "metadata": {},
   "source": [
    "## 图解stride\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/stride1d.svg\">\n",
    "<img src=\"./imgs/stride1d.svg\"\n",
    "    width=\"600\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6f3989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[-0.7747,  0.7926, -0.0062, -0.4377,  0.7148],\n",
      "         [ 0.3590, -0.1242,  2.0345, -0.3479, -0.4007]],\n",
      "\n",
      "        [[ 0.8059, -0.1021,  0.3168, -0.8889,  1.1768],\n",
      "         [ 0.8074,  0.9144,  1.6259, -0.6535, -0.0865]]])\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "output: tensor([[[-0.1682, -0.9502],\n",
      "         [-0.0725, -0.0527]],\n",
      "\n",
      "        [[-0.0756, -0.9533],\n",
      "         [-0.1843, -0.1548]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "print(\"input:\",input)\n",
    "\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=2, padding=0,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output) #可以看到output减少了一列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb726e73",
   "metadata": {},
   "source": [
    "## 图解padding\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/padding1d.svg\">\n",
    "<img src=\"./imgs/padding1d.svg\"\n",
    "    width=\"600\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9e4d84dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[-0.7747,  0.7926, -0.0062, -0.4377,  0.7148],\n",
      "         [ 0.3590, -0.1242,  2.0345, -0.3479, -0.4007]],\n",
      "\n",
      "        [[ 0.8059, -0.1021,  0.3168, -0.8889,  1.1768],\n",
      "         [ 0.8074,  0.9144,  1.6259, -0.6535, -0.0865]]])\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "output: tensor([[[ 0.0000,  0.0582,  0.0967, -0.1682,  0.7688, -0.9502, -0.0030,\n",
      "           0.0987,  0.0000],\n",
      "         [ 0.0000,  0.2905,  0.0201, -0.0725,  0.0577, -0.0527, -0.1947,\n",
      "          -0.1912,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.0708,  0.2678, -0.0756,  0.3155, -0.9533,  0.2494,\n",
      "          -0.0587,  0.0000],\n",
      "         [ 0.0000, -0.2852, -0.2170, -0.1843,  0.3324, -0.1548, -0.2527,\n",
      "          -0.2966,  0.0000]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "print(\"input:\",input)\n",
    "\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=3,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output) #随着padding的增加，输出的长度越来越长\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444f30",
   "metadata": {},
   "source": [
    "## 图解dilation\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/dilation1d.svg\">\n",
    "<img src=\"./imgs/dilation1d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6303de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[-0.7747,  0.7926, -0.0062, -0.4377,  0.7148],\n",
      "         [ 0.3590, -0.1242,  2.0345, -0.3479, -0.4007]],\n",
      "\n",
      "        [[ 0.8059, -0.1021,  0.3168, -0.8889,  1.1768],\n",
      "         [ 0.8074,  0.9144,  1.6259, -0.6535, -0.0865]]])\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "output: tensor([[[ 0.6142],\n",
      "         [ 0.0392]],\n",
      "\n",
      "        [[ 0.1173],\n",
      "         [-0.6504]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "print(\"input:\",input)\n",
    "\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=2, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e995900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61427946\n",
      "0.03914337000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_part = np.array([\n",
    "    [-0.7747, -0.0062, 0.7148],\n",
    "    [0.3590, 2.0345, -0.4007]\n",
    "])\n",
    "\n",
    "kernel_1 = np.array([\n",
    "    [-0.0783, -0.0347, -0.0791],\n",
    "    [-0.3860, 0.3662, -0.0087]\n",
    "])\n",
    "print(np.sum(input_part*kernel_1))\n",
    "\n",
    "kernel_2 = np.array([\n",
    "    [-0.2497, -0.3820, -0.3683],\n",
    "    [0.0319,  0.0496,  0.0144]\n",
    "])\n",
    "print(np.sum(input_part*kernel_2))\n",
    "# 结果和图上会有些许差异，主要是四舍五入所导致的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc4ea9",
   "metadata": {},
   "source": [
    "## 图解groups\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/groups1d.svg\">\n",
    "<img src=\"./imgs/groups1d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82679805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------groups=1----------------------:\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "output: tensor([[[-0.1682,  0.7688, -0.9502],\n",
      "         [-0.0725,  0.0577, -0.0527]],\n",
      "\n",
      "        [[-0.0756,  0.3155, -0.9533],\n",
      "         [-0.1843,  0.3324, -0.1548]]], grad_fn=<ConvolutionBackward0>)\n",
      "output.shape: torch.Size([2, 2, 3])\n",
      "\n",
      "----------------------groups=2----------------------:\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[ 0.2886, -0.0081, -0.0499]],\n",
      "\n",
      "        [[-0.2986, -0.1184, -0.2425]]], requires_grad=True)\n",
      "output: tensor([[[-0.2297,  0.2507, -0.0339],\n",
      "         [-0.5859, -0.1194, -0.4690]],\n",
      "\n",
      "        [[ 0.2177,  0.0123,  0.0400],\n",
      "         [-0.7437, -0.3071, -0.3871]]], grad_fn=<ConvolutionBackward0>)\n",
      "output.shape: torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "# print(\"input:\",input)\n",
    "\n",
    "print(\"\\n----------------------groups=1----------------------:\")\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output)\n",
    "print(\"output.shape:\",output.shape)\n",
    "\n",
    "print(\"\\n----------------------groups=2----------------------:\")\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=1, groups=2, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output)\n",
    "print(\"output.shape:\",output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a739f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.22968910000000003\n",
      "-0.58585837\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_part = np.array([\n",
    "    [-0.7747, 0.7926, -0.0062],\n",
    "    # [0.3590, -0.1242, 2.0345]\n",
    "])\n",
    "\n",
    "kernel_1 = np.array([\n",
    "    [0.2886, -0.0081, -0.0499]\n",
    "])\n",
    "print(np.sum(input_part*kernel_1))\n",
    "\n",
    "input_part = np.array([\n",
    "    # [-0.7747, 0.7926, -0.0062],\n",
    "    [0.3590, -0.1242, 2.0345]\n",
    "])\n",
    "\n",
    "kernel_2 = np.array([\n",
    "    [-0.2986, -0.1184, -0.2425]\n",
    "])\n",
    "print(np.sum(input_part*kernel_2))\n",
    "# 结果和图上会有些许差异，主要是四舍五入所导致的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601026d",
   "metadata": {},
   "source": [
    "## 图解bias\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/bias1d.svg\">\n",
    "<img src=\"./imgs/bias1d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92b6cfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[-0.7747,  0.7926, -0.0062, -0.4377,  0.7148],\n",
      "         [ 0.3590, -0.1242,  2.0345, -0.3479, -0.4007]],\n",
      "\n",
      "        [[ 0.8059, -0.1021,  0.3168, -0.8889,  1.1768],\n",
      "         [ 0.8074,  0.9144,  1.6259, -0.6535, -0.0865]]])\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "m.bias: Parameter containing:\n",
      "tensor([ 0.2041, -0.0057], requires_grad=True)\n",
      "output: tensor([[[ 0.0359,  0.9729, -0.7461],\n",
      "         [-0.0782,  0.0520, -0.0584]],\n",
      "\n",
      "        [[ 0.1285,  0.5196, -0.7492],\n",
      "         [-0.1901,  0.3266, -0.1605]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "print(\"input:\",input)\n",
    "\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "print(\"m.bias:\",m.bias)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac554c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035990019999999984\n",
      "-0.07815857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_part = np.array([\n",
    "    [-0.7747, 0.7926, -0.0062],\n",
    "    [0.3590, -0.1242, 2.0345]\n",
    "])\n",
    "\n",
    "kernel_1 = np.array([\n",
    "    [-0.0783, -0.0347, -0.0791],\n",
    "    [-0.3860, 0.3662, -0.0087]\n",
    "])\n",
    "print(np.sum(input_part*kernel_1)+0.2041)\n",
    "\n",
    "kernel_2 = np.array([\n",
    "    [-0.2497, -0.3820, -0.3683],\n",
    "    [0.0319,  0.0496,  0.0144]\n",
    "])\n",
    "print(np.sum(input_part*kernel_2)-0.0057)\n",
    "# 结果和图上会有些许差异，主要是四舍五入所导致的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5881bacc",
   "metadata": {},
   "source": [
    "# Conv2d\n",
    "对由多个输入平面组成的输入信号应用2D卷积。\n",
    "\n",
    "定义：  \n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "参数：  \n",
    "in_channels (int): Number of channels in the input image\n",
    "输入图像中的通道数\n",
    "\n",
    "out_channels (int): Number of channels produced by the convolution\n",
    "卷积产生的通道数\n",
    "\n",
    "kernel_size (int or tuple): Size of the convolving kernel\n",
    "卷积核的大小\n",
    "\n",
    "stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "卷积步幅。默认值:1\n",
    "\n",
    "padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: 0\n",
    "输入框4个边的添加的Padding尺寸。默认值:0\n",
    "\n",
    "padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
    "添加Padding的方式\n",
    "\n",
    "dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "卷积核元素之间的间距\n",
    "\n",
    "groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
    "分组卷积\n",
    "\n",
    "bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
    "添加一个可学习的偏置项\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e415a5c",
   "metadata": {},
   "source": [
    "## 图解in_channels、out_channels、kernel_size\n",
    "\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/conv2d.svg\">\n",
    "<img src=\"./imgs/conv2d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46634ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[[-0.7747,  0.7926, -0.0062, -0.4377,  0.4657],\n",
      "          [-0.1880, -0.8975,  0.4169, -0.3840,  0.0394],\n",
      "          [ 0.4869, -0.1476, -0.4459, -0.0336,  0.0221],\n",
      "          [-0.0550, -0.9645,  0.0285, -0.3170,  1.6640],\n",
      "          [ 0.7148,  0.3590, -0.1242,  2.0345,  0.9017]],\n",
      "\n",
      "         [[-1.1558,  0.1841,  0.0934,  0.3168, -0.8889],\n",
      "          [ 1.1768,  0.8074,  0.8133, -0.8232,  0.7238],\n",
      "          [ 1.3477,  0.9699, -1.0729, -0.9426, -0.2336],\n",
      "          [-0.2728,  0.0554,  1.9797,  0.2763,  0.3080],\n",
      "          [-0.2687, -0.3787, -0.0259, -1.6019,  0.1780]],\n",
      "\n",
      "         [[-0.7901, -0.2213, -0.9400,  0.4811, -0.5768],\n",
      "          [ 0.3937,  0.4542, -1.1519,  0.7723, -0.7097],\n",
      "          [-0.3601, -0.6734,  0.7011, -0.3622, -1.0503],\n",
      "          [-1.2473,  0.0234,  2.2317, -1.4306, -0.7068],\n",
      "          [ 0.3444, -0.9902, -0.9458,  0.8957,  0.5061]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7608,  0.4708,  1.3452,  0.6152, -0.4913],\n",
      "          [ 0.8625,  0.2028,  0.2190,  0.2907, -0.6460],\n",
      "          [-0.9671,  0.4261,  1.0192, -0.0523,  0.7585],\n",
      "          [ 1.3658,  1.9689, -0.6445, -1.2409,  0.0251],\n",
      "          [ 0.7908, -0.2810,  0.4610, -0.2212,  0.1101]],\n",
      "\n",
      "         [[-0.1352, -0.3804, -0.0035,  1.1548, -0.2004],\n",
      "          [-2.0605, -0.0994, -0.4985, -1.6124,  1.2166],\n",
      "          [ 0.5752,  1.3088, -1.1107,  0.1158, -0.4590],\n",
      "          [ 1.4115,  1.4704,  0.6992,  1.9934, -0.3557],\n",
      "          [-0.2348,  0.2263,  1.1475,  1.6159,  0.1001]],\n",
      "\n",
      "         [[ 1.0392,  0.0202,  0.1693,  0.9616,  0.5569],\n",
      "          [ 0.0225, -0.7353, -0.6873,  1.4160, -0.2969],\n",
      "          [ 1.1064,  0.6216,  2.5236, -1.3281,  0.4013],\n",
      "          [ 0.0764, -1.1287,  0.1072,  0.3902, -0.2689],\n",
      "          [-0.3125,  0.4083,  1.0081, -1.0820, -0.1143]]]])\n",
      "\n",
      "m.weight: torch.Size([2, 3, 3, 3])\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0762, -0.1053,  0.0464],\n",
      "          [-0.1129, -0.1230, -0.1107],\n",
      "          [-0.0667, -0.1036, -0.1620]],\n",
      "\n",
      "         [[ 0.0355, -0.1921, -0.0552],\n",
      "          [ 0.1444,  0.1322,  0.1732],\n",
      "          [ 0.0152, -0.0624,  0.0404]],\n",
      "\n",
      "         [[ 0.0104,  0.0387,  0.0363],\n",
      "          [ 0.0599,  0.0737, -0.0216],\n",
      "          [ 0.1002,  0.1255,  0.1053]]],\n",
      "\n",
      "\n",
      "        [[[-0.1619,  0.1820, -0.1211],\n",
      "          [ 0.1733,  0.1670, -0.0993],\n",
      "          [ 0.0322, -0.1547,  0.1175]],\n",
      "\n",
      "         [[-0.1096, -0.1338,  0.1487],\n",
      "          [-0.0573,  0.1454,  0.1511],\n",
      "          [ 0.1339, -0.0923, -0.1172]],\n",
      "\n",
      "         [[-0.0601, -0.0949,  0.0579],\n",
      "          [-0.1379,  0.0424,  0.1474],\n",
      "          [ 0.1801, -0.1381, -0.1706]]]], requires_grad=True)\n",
      "\n",
      "output: torch.Size([2, 2, 3, 3])\n",
      "tensor([[[[ 0.2347,  0.1779,  0.1566],\n",
      "          [ 0.2689, -0.1617, -0.2603],\n",
      "          [ 0.0243,  0.4554,  0.1814]],\n",
      "\n",
      "         [[ 0.2557,  0.2139,  0.0115],\n",
      "          [-1.0343, -0.6471,  0.6476],\n",
      "          [ 0.6038,  0.0814, -0.9786]]],\n",
      "\n",
      "\n",
      "        [[[-0.2327, -0.4715, -0.2302],\n",
      "          [-0.3154,  0.4881, -0.0841],\n",
      "          [ 0.0561,  0.8658,  0.4862]],\n",
      "\n",
      "         [[-0.3434,  0.4640,  0.5342],\n",
      "          [-0.1119, -0.5118,  0.3322],\n",
      "          [ 0.2051,  0.4320,  0.0109]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "N = 2  # batch_size\n",
    "C = 3  # 输入通道数\n",
    "H = 5  # 输入高度\n",
    "W = 5  # 输入宽度\n",
    "input = torch.randn(N, C, H, W)\n",
    "print(\"input:\", input)\n",
    "\n",
    "m = nn.Conv2d(in_channels=C, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"\\nm.weight:\", m.weight.shape)\n",
    "print(m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"\\noutput:\", output.shape)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bce0a2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23460186000000005\n",
      "0.25592334000000005\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_part = np.array([\n",
    "    [[-0.7747,  0.7926, -0.0062],\n",
    "     [-0.1880, -0.8975,  0.4169],\n",
    "     [0.4869, -0.1476, -0.4459]],\n",
    "\n",
    "    [[-1.1558,  0.1841,  0.0934],\n",
    "     [1.1768,  0.8074,  0.8133],\n",
    "     [1.3477,  0.9699, -1.0729]],\n",
    "\n",
    "    [[-0.7901, -0.2213, -0.9400],\n",
    "     [0.3937,  0.4542, -1.1519],\n",
    "     [-0.3601, -0.6734,  0.7011]]\n",
    "])\n",
    "\n",
    "kernel_1 = np.array([\n",
    "    [[0.0762, -0.1053,  0.0464],\n",
    "     [-0.1129, -0.1230, -0.1107],\n",
    "     [-0.0667, -0.1036, -0.1620]],\n",
    "\n",
    "    [[0.0355, -0.1921, -0.0552],\n",
    "     [0.1444,  0.1322,  0.1732],\n",
    "     [0.0152, -0.0624,  0.0404]],\n",
    "\n",
    "    [[0.0104,  0.0387,  0.0363],\n",
    "     [0.0599,  0.0737, -0.0216],\n",
    "     [0.1002,  0.1255,  0.1053]]\n",
    "])\n",
    "print(np.sum(input_part*kernel_1))\n",
    "\n",
    "kernel_2 = np.array([\n",
    "    [[-0.1619,  0.1820, -0.1211],\n",
    "     [0.1733,  0.1670, -0.0993],\n",
    "     [0.0322, -0.1547,  0.1175]],\n",
    "\n",
    "    [[-0.1096, -0.1338,  0.1487],\n",
    "     [-0.0573,  0.1454,  0.1511],\n",
    "     [0.1339, -0.0923, -0.1172]],\n",
    "\n",
    "    [[-0.0601, -0.0949,  0.0579],\n",
    "     [-0.1379,  0.0424,  0.1474],\n",
    "     [0.1801, -0.1381, -0.1706]]\n",
    "])\n",
    "print(np.sum(input_part*kernel_2))\n",
    "# 结果和图上会有些许差异，主要是四舍五入所导致的误差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03c889",
   "metadata": {},
   "source": [
    "## 图解stride\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/stride2d.svg\">\n",
    "<img src=\"./imgs/stride2d.svg\"\n",
    "    width=\"800\" /></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038c746",
   "metadata": {},
   "source": [
    "## 图解padding\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/padding2d.svg\">\n",
    "<img src=\"./imgs/padding2d.svg\"\n",
    "    width=\"800\" /></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a70c3",
   "metadata": {},
   "source": [
    "## 图解dilation\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/dilation2d.svg\">\n",
    "<img src=\"./imgs/dilation2d.svg\"\n",
    "    width=\"800\" /></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49e946",
   "metadata": {},
   "source": [
    "## 图解groups\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/groups2d.svg\">\n",
    "<img src=\"./imgs/groups2d.svg\"\n",
    "    width=\"800\" /></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c322fb3",
   "metadata": {},
   "source": [
    "# Conv3d\n",
    "对由多个输入平面组成的输入信号应用2D卷积。\n",
    "\n",
    "定义：   \n",
    "torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "参数：   \n",
    "in_channels (int): Number of channels in the input image\n",
    "输入图像中的通道数\n",
    "\n",
    "out_channels (int): Number of channels produced by the convolution\n",
    "卷积产生的通道数\n",
    "\n",
    "kernel_size (int or tuple): Size of the convolving kernel\n",
    "卷积核的大小\n",
    "\n",
    "stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "卷积步幅。默认值:1\n",
    "\n",
    "padding (int, tuple or str, optional): Padding added to all six sides of the input. Default: 0\n",
    "输入框6个边的添加的Padding尺寸。默认值:0\n",
    "\n",
    "padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
    "添加Padding的方式\n",
    "\n",
    "dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "卷积核元素之间的间距\n",
    "\n",
    "groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
    "分组卷积\n",
    "\n",
    "bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
    "添加一个可学习的偏置项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8a2d1",
   "metadata": {},
   "source": [
    "## 图解in_channels、out_channels、kernel_size\n",
    "\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/conv3d.svg\">\n",
    "<img src=\"./imgs/conv3d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "19044967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[[[-0.7747,  0.7926, -0.0062, -0.4377,  0.4657],\n",
      "           [-0.1880, -0.8975,  0.4169, -0.3840,  0.0394],\n",
      "           [ 0.4869, -0.1476, -0.4459, -0.0336,  0.0221],\n",
      "           [-0.0550, -0.9645,  0.0285, -0.3170,  1.6640],\n",
      "           [ 0.7148,  0.3590, -0.1242,  2.0345,  0.9017]],\n",
      "\n",
      "          [[-1.1558,  0.1841,  0.0934,  0.3168, -0.8889],\n",
      "           [ 1.1768,  0.8074,  0.8133, -0.8232,  0.7238],\n",
      "           [ 1.3477,  0.9699, -1.0729, -0.9426, -0.2336],\n",
      "           [-0.2728,  0.0554,  1.9797,  0.2763,  0.3080],\n",
      "           [-0.2687, -0.3787, -0.0259, -1.6019,  0.1780]],\n",
      "\n",
      "          [[-0.7901, -0.2213, -0.9400,  0.4811, -0.5768],\n",
      "           [ 0.3937,  0.4542, -1.1519,  0.7723, -0.7097],\n",
      "           [-0.3601, -0.6734,  0.7011, -0.3622, -1.0503],\n",
      "           [-1.2473,  0.0234,  2.2317, -1.4306, -0.7068],\n",
      "           [ 0.3444, -0.9902, -0.9458,  0.8957,  0.5061]],\n",
      "\n",
      "          [[ 0.7608,  0.4708,  1.3452,  0.6152, -0.4913],\n",
      "           [ 0.8625,  0.2028,  0.2190,  0.2907, -0.6460],\n",
      "           [-0.9671,  0.4261,  1.0192, -0.0523,  0.7585],\n",
      "           [ 1.3658,  1.9689, -0.6445, -1.2409,  0.0251],\n",
      "           [ 0.7908, -0.2810,  0.4610, -0.2212,  0.1101]]],\n",
      "\n",
      "\n",
      "         [[[-0.1352, -0.3804, -0.0035,  1.1548, -0.2004],\n",
      "           [-2.0605, -0.0994, -0.4985, -1.6124,  1.2166],\n",
      "           [ 0.5752,  1.3088, -1.1107,  0.1158, -0.4590],\n",
      "           [ 1.4115,  1.4704,  0.6992,  1.9934, -0.3557],\n",
      "           [-0.2348,  0.2263,  1.1475,  1.6159,  0.1001]],\n",
      "\n",
      "          [[ 1.0392,  0.0202,  0.1693,  0.9616,  0.5569],\n",
      "           [ 0.0225, -0.7353, -0.6873,  1.4160, -0.5489],\n",
      "           [-1.4361, -1.1645, -0.3587, -0.8193, -0.4763],\n",
      "           [ 0.0661, -0.1041, -0.2091, -0.5399,  0.3606],\n",
      "           [ 1.9838, -0.6058, -3.1444, -0.4592, -1.1220]],\n",
      "\n",
      "          [[-0.2969,  1.1064,  1.2398, -0.5019, -0.5625],\n",
      "           [ 1.1341,  0.1663,  0.1811,  0.1072,  0.3902],\n",
      "           [-1.6994,  0.7552,  0.4651,  0.1519, -0.7855],\n",
      "           [ 0.1457,  1.3601, -0.5992, -1.6061,  2.7006],\n",
      "           [ 0.9943,  0.6306,  1.4991,  1.1960,  0.7380]],\n",
      "\n",
      "          [[-0.3928, -0.0423, -0.7581, -1.6920, -1.0772],\n",
      "           [-0.8806, -1.1690,  0.0592,  0.6268, -0.0072],\n",
      "           [-0.5542, -1.1403, -1.5966, -2.2841,  0.4298],\n",
      "           [-0.9062, -1.2104,  1.4074,  0.1383,  0.5804],\n",
      "           [ 0.3687,  1.8518, -1.3826,  0.5351,  1.0329]]],\n",
      "\n",
      "\n",
      "         [[[ 0.9951, -0.3827,  2.6233,  0.5235, -1.6050],\n",
      "           [ 1.8769, -0.5177, -0.8276,  1.4394,  0.7606],\n",
      "           [ 0.4166,  0.8600, -0.5446, -0.4660, -0.5025],\n",
      "           [ 1.3622, -1.3012, -0.1548,  0.5115,  0.3209],\n",
      "           [ 0.5364, -1.3732, -0.2308,  0.1688, -1.2982]],\n",
      "\n",
      "          [[ 0.0806,  1.2157,  0.9629, -0.6228,  0.1235],\n",
      "           [ 1.1248,  0.9178,  0.3492,  0.8592,  0.9008],\n",
      "           [ 0.3955, -1.1087, -0.5244, -0.0417,  1.7403],\n",
      "           [ 2.0217,  0.5893,  2.8736,  1.0473,  3.1900],\n",
      "           [ 0.0309, -0.5275, -1.3201, -1.1506, -0.8146]],\n",
      "\n",
      "          [[ 0.1766, -0.2803,  0.7043, -0.4091,  2.2282],\n",
      "           [-0.3709,  1.1516,  1.8171, -0.6805,  0.6680],\n",
      "           [-0.3194, -2.5387,  0.2613,  0.9761, -0.3623],\n",
      "           [-2.0083,  1.4599,  1.6927,  0.2498, -0.7959],\n",
      "           [-0.9650,  0.8382,  0.0157, -0.6284, -0.7056]],\n",
      "\n",
      "          [[ 0.0432, -0.5965,  1.3597, -1.1757, -0.0490],\n",
      "           [-0.0714,  0.3247,  0.5243, -1.9872, -0.6623],\n",
      "           [ 0.9260, -1.4672,  1.1317,  0.1165,  0.4261],\n",
      "           [ 0.1918, -0.1621, -0.5467,  0.7244, -1.1876],\n",
      "           [-0.3271, -0.1753,  1.3208,  0.1476,  1.8017]]]]])\n",
      "\n",
      "m.weight: torch.Size([1, 3, 4, 4, 4])\n",
      "Parameter containing:\n",
      "tensor([[[[[-4.2748e-04, -1.4728e-02, -4.5107e-03, -3.1408e-02],\n",
      "           [ 2.5778e-02, -2.2399e-02,  1.2452e-02, -5.6903e-02],\n",
      "           [-4.6099e-02,  3.7366e-02,  1.5147e-02, -2.9998e-02],\n",
      "           [-5.1247e-02,  6.4096e-02, -2.0607e-02,  1.3345e-02]],\n",
      "\n",
      "          [[ 4.4371e-02, -2.5049e-02,  6.2656e-02,  4.3595e-02],\n",
      "           [-2.9082e-02,  1.9971e-03, -3.5772e-02,  6.6125e-02],\n",
      "           [-5.3342e-02,  5.3335e-02, -3.8501e-02, -4.2106e-02],\n",
      "           [-2.4264e-02,  4.8827e-02,  2.9670e-02, -6.4108e-02]],\n",
      "\n",
      "          [[-4.9841e-02,  5.9055e-02, -2.3933e-02, -3.8853e-02],\n",
      "           [-2.1447e-02, -1.6101e-03, -4.3575e-02,  7.1368e-02],\n",
      "           [ 2.7772e-02,  3.5428e-02,  1.6072e-02, -7.0194e-02],\n",
      "           [ 2.7474e-02, -4.1947e-02,  2.3789e-02,  6.0283e-02]],\n",
      "\n",
      "          [[ 3.4243e-02, -3.6311e-02,  4.6370e-03,  2.5527e-02],\n",
      "           [ 6.0106e-02, -5.4526e-02, -4.4994e-02,  5.3184e-02],\n",
      "           [-6.3586e-02, -4.4123e-03,  4.1517e-02,  1.7920e-02],\n",
      "           [ 5.8563e-02, -1.6998e-02, -1.1812e-02, -6.2997e-02]]],\n",
      "\n",
      "\n",
      "         [[[-2.3538e-02, -5.0025e-02, -2.0850e-03, -1.9331e-02],\n",
      "           [ 5.7346e-02, -6.5427e-02,  1.2749e-02, -4.4134e-02],\n",
      "           [ 5.0442e-02, -3.4745e-02, -4.1959e-02,  2.8299e-02],\n",
      "           [ 6.3810e-03, -5.7453e-02, -8.4617e-03, -4.5681e-02]],\n",
      "\n",
      "          [[-4.9313e-03,  2.3213e-03,  1.3892e-02, -1.8015e-02],\n",
      "           [-5.8533e-02, -6.3097e-03,  3.6705e-02,  6.2551e-02],\n",
      "           [ 6.4893e-02,  3.2187e-02,  2.3547e-02, -4.0034e-02],\n",
      "           [ 3.1267e-02,  4.2968e-02,  5.9388e-02, -8.3630e-03]],\n",
      "\n",
      "          [[ 3.7797e-02,  2.4875e-02, -2.1732e-02,  8.1970e-03],\n",
      "           [ 7.0882e-02,  5.4264e-02, -3.5409e-02,  5.4529e-02],\n",
      "           [ 4.5896e-02, -3.8101e-02, -3.7950e-02, -7.1806e-02],\n",
      "           [-3.6849e-02,  3.2856e-02, -4.9928e-03, -1.6977e-02]],\n",
      "\n",
      "          [[ 4.2963e-02,  1.5525e-02, -1.1051e-02,  2.6180e-02],\n",
      "           [ 2.8859e-02, -3.5611e-02, -2.9514e-03, -3.1140e-02],\n",
      "           [ 4.4017e-02,  4.7542e-05, -2.6098e-02, -1.9676e-02],\n",
      "           [-6.6095e-03,  6.1406e-02,  5.9758e-02,  6.0025e-02]]],\n",
      "\n",
      "\n",
      "         [[[ 9.7614e-03,  3.2088e-03,  5.0668e-02,  2.7365e-02],\n",
      "           [ 2.4458e-02, -5.1479e-02,  3.8940e-02, -3.4862e-03],\n",
      "           [ 6.6586e-02,  8.5235e-03,  5.1033e-02, -5.9170e-02],\n",
      "           [-6.1563e-02,  1.8730e-02,  1.4484e-02, -3.3860e-02]],\n",
      "\n",
      "          [[-6.1930e-02, -4.7992e-03,  6.0117e-02, -4.0145e-02],\n",
      "           [ 5.4037e-02,  8.8125e-03, -2.2297e-03, -4.9141e-02],\n",
      "           [ 3.7618e-02,  4.3882e-02,  7.0411e-02, -2.9588e-02],\n",
      "           [-4.8736e-02, -5.3627e-02, -2.4342e-02, -1.8106e-02]],\n",
      "\n",
      "          [[ 1.6228e-02,  1.5861e-02,  4.1779e-02,  5.2704e-02],\n",
      "           [ 6.6622e-02,  5.6767e-02,  4.7262e-02,  6.7696e-02],\n",
      "           [-6.4517e-02,  4.5227e-02,  4.6826e-02,  3.2200e-03],\n",
      "           [-1.5336e-02, -3.2087e-02,  6.7092e-02,  1.5729e-02]],\n",
      "\n",
      "          [[-2.5609e-02,  1.3618e-02, -1.2973e-02, -3.6128e-02],\n",
      "           [ 3.8706e-02, -1.9684e-02, -5.2898e-02,  2.4190e-02],\n",
      "           [-5.5462e-02, -6.5853e-02,  5.1445e-03, -7.0777e-02],\n",
      "           [ 4.2291e-02,  2.1120e-02, -2.9539e-02,  2.9790e-02]]]]],\n",
      "       requires_grad=True)\n",
      "\n",
      "output: torch.Size([1, 1, 1, 2, 2])\n",
      "tensor([[[[[ 0.0917,  0.1903],\n",
      "           [ 0.2695, -0.1850]]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "N = 1  # batch_size\n",
    "C = 3  # 输入通道数\n",
    "D = 4  # 输入深度\n",
    "H = 5  # 输入高度\n",
    "W = 5  # 输入宽度\n",
    "input = torch.randn(N, C, D, H, W) # 假设每次操作处理3张RGBA图像，C表示3张，D表示RGBA的深度即4.\n",
    "print(\"input:\", input)\n",
    "\n",
    "m = nn.Conv3d(in_channels=C, out_channels=1, kernel_size=4, stride=1, padding=0,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"\\nm.weight:\", m.weight.shape)\n",
    "print(m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"\\noutput:\", output.shape)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d4344f",
   "metadata": {},
   "source": [
    "## 图解padding\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/padding3d.svg\">\n",
    "<img src=\"./imgs/padding3d.svg\"\n",
    "    width=\"800\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c96d39fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 3, 4, 5, 5])\n",
      "\n",
      "m.weight: torch.Size([1, 3, 4, 4, 4])\n",
      "\n",
      "output: torch.Size([1, 1, 3, 4, 4])\n",
      "tensor([[[[[ 0.0300,  0.1076, -0.0497,  0.3170],\n",
      "           [ 0.6031,  0.5510,  0.8819,  0.3251],\n",
      "           [ 0.4036, -0.2012,  0.1180,  0.4141],\n",
      "           [ 0.9381, -0.2797,  0.3780,  0.7740]],\n",
      "\n",
      "          [[ 0.0768, -0.0678, -0.1671,  0.1202],\n",
      "           [-0.1983,  0.0917,  0.1903, -0.3414],\n",
      "           [ 0.1599,  0.2695, -0.1850,  0.6414],\n",
      "           [-0.5457, -0.7288, -0.2404, -0.7551]],\n",
      "\n",
      "          [[ 0.5248,  0.6759, -0.4467, -0.0841],\n",
      "           [-0.5097, -0.4047, -0.0880, -0.4256],\n",
      "           [ 0.3346,  0.8154,  0.0728,  0.1772],\n",
      "           [-0.4890,  0.1221,  0.3521,  0.1255]]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "N = 1  # batch_size\n",
    "C = 3  # 输入通道数\n",
    "D = 4  # 输入深度\n",
    "H = 5  # 输入高度\n",
    "W = 5  # 输入宽度\n",
    "input = torch.randn(N, C, D, H, W) # 假设每次操作处理3张RGBA图像，C表示3张，D表示RGBA的深度即4.\n",
    "# print(\"input:\", input)\n",
    "print(\"input:\",input.shape)\n",
    "\n",
    "m = nn.Conv3d(in_channels=C, out_channels=1, kernel_size=4, stride=1, padding=1,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "# print(\"\\nm.weight:\", m.weight)\n",
    "print(\"\\nm.weight:\",m.weight.shape)\n",
    "\n",
    "output = m(input)\n",
    "print(\"\\noutput:\", output.shape)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad04c44",
   "metadata": {},
   "source": [
    "## stride、dilation、groups和1/2d同理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78102bae",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "\n",
    "[《人工智能导论：模型与算法》](https://item.jd.com/12653461.html)\n",
    "\n",
    "[《统计学习方法》](https://item.jd.com/12522197.html)\n",
    "\n",
    "[《机器学习》](https://item.jd.com/12762673.html)\n",
    "\n",
    "[FIVE KEY ASSUMPTIONS OF LINEAR REGRESSION ALGORITHM](https://dataaspirant.com/assumptions-of-linear-regression-algorithm/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e00c79739f2fdf113306667eb0b8e68d4274855301e6df90bc305a954991b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
