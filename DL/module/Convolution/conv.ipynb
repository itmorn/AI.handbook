{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e625b5",
   "metadata": {},
   "source": [
    "[![](/imgs/colab-badge.svg)](https://colab.research.google.com/github/itmorn/AI.handbook/blob/main/DL/module/Convolution/conv.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8662b",
   "metadata": {},
   "source": [
    "# Conv1d\n",
    "对由多个输入平面组成的输入信号应用1D卷积。\n",
    "\n",
    "定义：  \n",
    "torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "参数：  \n",
    "in_channels (int): Number of channels in the input image\n",
    "输入图像中的通道数\n",
    "\n",
    "out_channels (int): Number of channels produced by the convolution\n",
    "卷积产生的通道数\n",
    "\n",
    "kernel_size (int or tuple): Size of the convolving kernel\n",
    "卷积核的大小\n",
    "\n",
    "stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "卷积步幅。默认值:1\n",
    "\n",
    "padding (int, tuple or str, optional): Padding added to both sides of the input. Default: 0\n",
    "输入框两边的添加的Padding尺寸。默认值:0\n",
    "\n",
    "padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
    "添加Padding的方式\n",
    "\n",
    "dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "卷积核元素之间的间距\n",
    "\n",
    "groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
    "分组卷积\n",
    "\n",
    "bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
    "添加一个可学习的偏置项\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006d014",
   "metadata": {},
   "source": [
    "## 图解in_channels、out_channels、kernel_size\n",
    "\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/conv1d.svg\">\n",
    "<img src=\"./imgs/conv1d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05f9d3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[-0.7747,  0.7926, -0.0062, -0.4377,  0.7148],\n",
      "         [ 0.3590, -0.1242,  2.0345, -0.3479, -0.4007]],\n",
      "\n",
      "        [[ 0.8059, -0.1021,  0.3168, -0.8889,  1.1768],\n",
      "         [ 0.8074,  0.9144,  1.6259, -0.6535, -0.0865]]])\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "output: tensor([[[-0.1682,  0.7688, -0.9502],\n",
      "         [-0.0725,  0.0577, -0.0527]],\n",
      "\n",
      "        [[-0.0756,  0.3155, -0.9533],\n",
      "         [-0.1843,  0.3324, -0.1548]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "print(\"input:\",input)\n",
    "\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4680baca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.16810998000000002\n",
      "-0.07245857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_part = np.array([\n",
    "    [-0.7747, 0.7926, -0.0062],\n",
    "    [0.3590, -0.1242, 2.0345]\n",
    "])\n",
    "\n",
    "kernel_1 = np.array([\n",
    "    [-0.0783, -0.0347, -0.0791],\n",
    "    [-0.3860, 0.3662, -0.0087]\n",
    "])\n",
    "print(np.sum(input_part*kernel_1))\n",
    "\n",
    "kernel_2 = np.array([\n",
    "    [-0.2497, -0.3820, -0.3683],\n",
    "    [0.0319,  0.0496,  0.0144]\n",
    "])\n",
    "print(np.sum(input_part*kernel_2))\n",
    "# 结果和图上会有些许差异，主要是四舍五入所导致的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75607d01",
   "metadata": {},
   "source": [
    "## 图解stride\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/stride1d.svg\">\n",
    "<img src=\"./imgs/stride1d.svg\"\n",
    "    width=\"600\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6f3989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[-0.7747,  0.7926, -0.0062, -0.4377,  0.7148],\n",
      "         [ 0.3590, -0.1242,  2.0345, -0.3479, -0.4007]],\n",
      "\n",
      "        [[ 0.8059, -0.1021,  0.3168, -0.8889,  1.1768],\n",
      "         [ 0.8074,  0.9144,  1.6259, -0.6535, -0.0865]]])\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "output: tensor([[[-0.1682, -0.9502],\n",
      "         [-0.0725, -0.0527]],\n",
      "\n",
      "        [[-0.0756, -0.9533],\n",
      "         [-0.1843, -0.1548]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "print(\"input:\",input)\n",
    "\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=2, padding=0,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output) #可以看到output减少了一列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb726e73",
   "metadata": {},
   "source": [
    "## 图解padding\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/padding1d.svg\">\n",
    "<img src=\"./imgs/padding1d.svg\"\n",
    "    width=\"600\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e4d84dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[-0.7747,  0.7926, -0.0062, -0.4377,  0.7148],\n",
      "         [ 0.3590, -0.1242,  2.0345, -0.3479, -0.4007]],\n",
      "\n",
      "        [[ 0.8059, -0.1021,  0.3168, -0.8889,  1.1768],\n",
      "         [ 0.8074,  0.9144,  1.6259, -0.6535, -0.0865]]])\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "output: tensor([[[ 0.0000,  0.0582,  0.0967, -0.1682,  0.7688, -0.9502, -0.0030,\n",
      "           0.0987,  0.0000],\n",
      "         [ 0.0000,  0.2905,  0.0201, -0.0725,  0.0577, -0.0527, -0.1947,\n",
      "          -0.1912,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.0708,  0.2678, -0.0756,  0.3155, -0.9533,  0.2494,\n",
      "          -0.0587,  0.0000],\n",
      "         [ 0.0000, -0.2852, -0.2170, -0.1843,  0.3324, -0.1548, -0.2527,\n",
      "          -0.2966,  0.0000]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "print(\"input:\",input)\n",
    "\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=3,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output) #随着padding的增加，输出的长度越来越长\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444f30",
   "metadata": {},
   "source": [
    "## 图解dilation\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/dilation1d.svg\">\n",
    "<img src=\"./imgs/dilation1d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6303de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[-0.7747,  0.7926, -0.0062, -0.4377,  0.7148],\n",
      "         [ 0.3590, -0.1242,  2.0345, -0.3479, -0.4007]],\n",
      "\n",
      "        [[ 0.8059, -0.1021,  0.3168, -0.8889,  1.1768],\n",
      "         [ 0.8074,  0.9144,  1.6259, -0.6535, -0.0865]]])\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "output: tensor([[[ 0.6142],\n",
      "         [ 0.0392]],\n",
      "\n",
      "        [[ 0.1173],\n",
      "         [-0.6504]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "print(\"input:\",input)\n",
    "\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=2, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e995900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61427946\n",
      "0.03914337000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_part = np.array([\n",
    "    [-0.7747, -0.0062, 0.7148],\n",
    "    [0.3590, 2.0345, -0.4007]\n",
    "])\n",
    "\n",
    "kernel_1 = np.array([\n",
    "    [-0.0783, -0.0347, -0.0791],\n",
    "    [-0.3860, 0.3662, -0.0087]\n",
    "])\n",
    "print(np.sum(input_part*kernel_1))\n",
    "\n",
    "kernel_2 = np.array([\n",
    "    [-0.2497, -0.3820, -0.3683],\n",
    "    [0.0319,  0.0496,  0.0144]\n",
    "])\n",
    "print(np.sum(input_part*kernel_2))\n",
    "# 结果和图上会有些许差异，主要是四舍五入所导致的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc4ea9",
   "metadata": {},
   "source": [
    "## 图解groups\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/groups1d.svg\">\n",
    "<img src=\"./imgs/groups1d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82679805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------groups=1----------------------:\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "output: tensor([[[-0.1682,  0.7688, -0.9502],\n",
      "         [-0.0725,  0.0577, -0.0527]],\n",
      "\n",
      "        [[-0.0756,  0.3155, -0.9533],\n",
      "         [-0.1843,  0.3324, -0.1548]]], grad_fn=<ConvolutionBackward0>)\n",
      "output.shape: torch.Size([2, 2, 3])\n",
      "\n",
      "----------------------groups=2----------------------:\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[ 0.2886, -0.0081, -0.0499]],\n",
      "\n",
      "        [[-0.2986, -0.1184, -0.2425]]], requires_grad=True)\n",
      "output: tensor([[[-0.2297,  0.2507, -0.0339],\n",
      "         [-0.5859, -0.1194, -0.4690]],\n",
      "\n",
      "        [[ 0.2177,  0.0123,  0.0400],\n",
      "         [-0.7437, -0.3071, -0.3871]]], grad_fn=<ConvolutionBackward0>)\n",
      "output.shape: torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "# print(\"input:\",input)\n",
    "\n",
    "print(\"\\n----------------------groups=1----------------------:\")\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output)\n",
    "print(\"output.shape:\",output.shape)\n",
    "\n",
    "print(\"\\n----------------------groups=2----------------------:\")\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=1, groups=2, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output)\n",
    "print(\"output.shape:\",output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a739f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.22968910000000003\n",
      "-0.58585837\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_part = np.array([\n",
    "    [-0.7747, 0.7926, -0.0062],\n",
    "    # [0.3590, -0.1242, 2.0345]\n",
    "])\n",
    "\n",
    "kernel_1 = np.array([\n",
    "    [0.2886, -0.0081, -0.0499]\n",
    "])\n",
    "print(np.sum(input_part*kernel_1))\n",
    "\n",
    "input_part = np.array([\n",
    "    # [-0.7747, 0.7926, -0.0062],\n",
    "    [0.3590, -0.1242, 2.0345]\n",
    "])\n",
    "\n",
    "kernel_2 = np.array([\n",
    "    [-0.2986, -0.1184, -0.2425]\n",
    "])\n",
    "print(np.sum(input_part*kernel_2))\n",
    "# 结果和图上会有些许差异，主要是四舍五入所导致的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601026d",
   "metadata": {},
   "source": [
    "## 图解bias\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/bias1d.svg\">\n",
    "<img src=\"./imgs/bias1d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92b6cfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[-0.7747,  0.7926, -0.0062, -0.4377,  0.7148],\n",
      "         [ 0.3590, -0.1242,  2.0345, -0.3479, -0.4007]],\n",
      "\n",
      "        [[ 0.8059, -0.1021,  0.3168, -0.8889,  1.1768],\n",
      "         [ 0.8074,  0.9144,  1.6259, -0.6535, -0.0865]]])\n",
      "m.weight: Parameter containing:\n",
      "tensor([[[-0.0783, -0.0347, -0.0791],\n",
      "         [-0.3860,  0.3662, -0.0087]],\n",
      "\n",
      "        [[-0.2497, -0.3820, -0.3683],\n",
      "         [ 0.0319,  0.0496,  0.0144]]], requires_grad=True)\n",
      "m.bias: Parameter containing:\n",
      "tensor([ 0.2041, -0.0057], requires_grad=True)\n",
      "output: tensor([[[ 0.0359,  0.9729, -0.7461],\n",
      "         [-0.0782,  0.0520, -0.0584]],\n",
      "\n",
      "        [[ 0.1285,  0.5196, -0.7492],\n",
      "         [-0.1901,  0.3266, -0.1605]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 2\n",
    "sequence_length = 5\n",
    "input = torch.randn(batch_size, in_channels, sequence_length)\n",
    "print(\"input:\",input)\n",
    "\n",
    "m = nn.Conv1d(in_channels=in_channels, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"m.weight:\",m.weight)\n",
    "print(\"m.bias:\",m.bias)\n",
    "\n",
    "output = m(input)\n",
    "print(\"output:\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac554c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035990019999999984\n",
      "-0.07815857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_part = np.array([\n",
    "    [-0.7747, 0.7926, -0.0062],\n",
    "    [0.3590, -0.1242, 2.0345]\n",
    "])\n",
    "\n",
    "kernel_1 = np.array([\n",
    "    [-0.0783, -0.0347, -0.0791],\n",
    "    [-0.3860, 0.3662, -0.0087]\n",
    "])\n",
    "print(np.sum(input_part*kernel_1)+0.2041)\n",
    "\n",
    "kernel_2 = np.array([\n",
    "    [-0.2497, -0.3820, -0.3683],\n",
    "    [0.0319,  0.0496,  0.0144]\n",
    "])\n",
    "print(np.sum(input_part*kernel_2)-0.0057)\n",
    "# 结果和图上会有些许差异，主要是四舍五入所导致的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5881bacc",
   "metadata": {},
   "source": [
    "# Conv2d\n",
    "对由多个输入平面组成的输入信号应用2D卷积。\n",
    "\n",
    "定义：  \n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "参数：  \n",
    "in_channels (int): Number of channels in the input image\n",
    "输入图像中的通道数\n",
    "\n",
    "out_channels (int): Number of channels produced by the convolution\n",
    "卷积产生的通道数\n",
    "\n",
    "kernel_size (int or tuple): Size of the convolving kernel\n",
    "卷积核的大小\n",
    "\n",
    "stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "卷积步幅。默认值:1\n",
    "\n",
    "padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: 0\n",
    "输入框四个边的添加的Padding尺寸。默认值:0\n",
    "\n",
    "padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
    "添加Padding的方式\n",
    "\n",
    "dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "卷积核元素之间的间距\n",
    "\n",
    "groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
    "分组卷积\n",
    "\n",
    "bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
    "添加一个可学习的偏置项\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e415a5c",
   "metadata": {},
   "source": [
    "## 图解in_channels、out_channels、kernel_size\n",
    "\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/conv2d.svg\">\n",
    "<img src=\"./imgs/conv2d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46634ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[[[-0.7747,  0.7926, -0.0062, -0.4377,  0.4657],\n",
      "          [-0.1880, -0.8975,  0.4169, -0.3840,  0.0394],\n",
      "          [ 0.4869, -0.1476, -0.4459, -0.0336,  0.0221],\n",
      "          [-0.0550, -0.9645,  0.0285, -0.3170,  1.6640],\n",
      "          [ 0.7148,  0.3590, -0.1242,  2.0345,  0.9017]],\n",
      "\n",
      "         [[-1.1558,  0.1841,  0.0934,  0.3168, -0.8889],\n",
      "          [ 1.1768,  0.8074,  0.8133, -0.8232,  0.7238],\n",
      "          [ 1.3477,  0.9699, -1.0729, -0.9426, -0.2336],\n",
      "          [-0.2728,  0.0554,  1.9797,  0.2763,  0.3080],\n",
      "          [-0.2687, -0.3787, -0.0259, -1.6019,  0.1780]],\n",
      "\n",
      "         [[-0.7901, -0.2213, -0.9400,  0.4811, -0.5768],\n",
      "          [ 0.3937,  0.4542, -1.1519,  0.7723, -0.7097],\n",
      "          [-0.3601, -0.6734,  0.7011, -0.3622, -1.0503],\n",
      "          [-1.2473,  0.0234,  2.2317, -1.4306, -0.7068],\n",
      "          [ 0.3444, -0.9902, -0.9458,  0.8957,  0.5061]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7608,  0.4708,  1.3452,  0.6152, -0.4913],\n",
      "          [ 0.8625,  0.2028,  0.2190,  0.2907, -0.6460],\n",
      "          [-0.9671,  0.4261,  1.0192, -0.0523,  0.7585],\n",
      "          [ 1.3658,  1.9689, -0.6445, -1.2409,  0.0251],\n",
      "          [ 0.7908, -0.2810,  0.4610, -0.2212,  0.1101]],\n",
      "\n",
      "         [[-0.1352, -0.3804, -0.0035,  1.1548, -0.2004],\n",
      "          [-2.0605, -0.0994, -0.4985, -1.6124,  1.2166],\n",
      "          [ 0.5752,  1.3088, -1.1107,  0.1158, -0.4590],\n",
      "          [ 1.4115,  1.4704,  0.6992,  1.9934, -0.3557],\n",
      "          [-0.2348,  0.2263,  1.1475,  1.6159,  0.1001]],\n",
      "\n",
      "         [[ 1.0392,  0.0202,  0.1693,  0.9616,  0.5569],\n",
      "          [ 0.0225, -0.7353, -0.6873,  1.4160, -0.2969],\n",
      "          [ 1.1064,  0.6216,  2.5236, -1.3281,  0.4013],\n",
      "          [ 0.0764, -1.1287,  0.1072,  0.3902, -0.2689],\n",
      "          [-0.3125,  0.4083,  1.0081, -1.0820, -0.1143]]]])\n",
      "\n",
      "m.weight: torch.Size([2, 3, 3, 3])\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0762, -0.1053,  0.0464],\n",
      "          [-0.1129, -0.1230, -0.1107],\n",
      "          [-0.0667, -0.1036, -0.1620]],\n",
      "\n",
      "         [[ 0.0355, -0.1921, -0.0552],\n",
      "          [ 0.1444,  0.1322,  0.1732],\n",
      "          [ 0.0152, -0.0624,  0.0404]],\n",
      "\n",
      "         [[ 0.0104,  0.0387,  0.0363],\n",
      "          [ 0.0599,  0.0737, -0.0216],\n",
      "          [ 0.1002,  0.1255,  0.1053]]],\n",
      "\n",
      "\n",
      "        [[[-0.1619,  0.1820, -0.1211],\n",
      "          [ 0.1733,  0.1670, -0.0993],\n",
      "          [ 0.0322, -0.1547,  0.1175]],\n",
      "\n",
      "         [[-0.1096, -0.1338,  0.1487],\n",
      "          [-0.0573,  0.1454,  0.1511],\n",
      "          [ 0.1339, -0.0923, -0.1172]],\n",
      "\n",
      "         [[-0.0601, -0.0949,  0.0579],\n",
      "          [-0.1379,  0.0424,  0.1474],\n",
      "          [ 0.1801, -0.1381, -0.1706]]]], requires_grad=True)\n",
      "\n",
      "output: torch.Size([2, 2, 3, 3])\n",
      "tensor([[[[ 0.2347,  0.1779,  0.1566],\n",
      "          [ 0.2689, -0.1617, -0.2603],\n",
      "          [ 0.0243,  0.4554,  0.1814]],\n",
      "\n",
      "         [[ 0.2557,  0.2139,  0.0115],\n",
      "          [-1.0343, -0.6471,  0.6476],\n",
      "          [ 0.6038,  0.0814, -0.9786]]],\n",
      "\n",
      "\n",
      "        [[[-0.2327, -0.4715, -0.2302],\n",
      "          [-0.3154,  0.4881, -0.0841],\n",
      "          [ 0.0561,  0.8658,  0.4862]],\n",
      "\n",
      "         [[-0.3434,  0.4640,  0.5342],\n",
      "          [-0.1119, -0.5118,  0.3322],\n",
      "          [ 0.2051,  0.4320,  0.0109]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(666)\n",
    "\n",
    "N = 2  # batch_size\n",
    "C = 3  # 输入通道数\n",
    "H = 5  # 输入高度\n",
    "W = 5  # 输入宽度\n",
    "input = torch.randn(N, C, H, W)\n",
    "print(\"input:\", input)\n",
    "\n",
    "m = nn.Conv2d(in_channels=C, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "              dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "print(\"\\nm.weight:\", m.weight.shape)\n",
    "print(m.weight)\n",
    "\n",
    "output = m(input)\n",
    "print(\"\\noutput:\", output.shape)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bce0a2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23460186000000005\n",
      "0.25592334000000005\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_part = np.array([\n",
    "    [[-0.7747,  0.7926, -0.0062],\n",
    "     [-0.1880, -0.8975,  0.4169],\n",
    "     [0.4869, -0.1476, -0.4459]],\n",
    "\n",
    "    [[-1.1558,  0.1841,  0.0934],\n",
    "     [1.1768,  0.8074,  0.8133],\n",
    "     [1.3477,  0.9699, -1.0729]],\n",
    "\n",
    "    [[-0.7901, -0.2213, -0.9400],\n",
    "     [0.3937,  0.4542, -1.1519],\n",
    "     [-0.3601, -0.6734,  0.7011]]\n",
    "])\n",
    "\n",
    "kernel_1 = np.array([\n",
    "    [[0.0762, -0.1053,  0.0464],\n",
    "     [-0.1129, -0.1230, -0.1107],\n",
    "     [-0.0667, -0.1036, -0.1620]],\n",
    "\n",
    "    [[0.0355, -0.1921, -0.0552],\n",
    "     [0.1444,  0.1322,  0.1732],\n",
    "     [0.0152, -0.0624,  0.0404]],\n",
    "\n",
    "    [[0.0104,  0.0387,  0.0363],\n",
    "     [0.0599,  0.0737, -0.0216],\n",
    "     [0.1002,  0.1255,  0.1053]]\n",
    "])\n",
    "print(np.sum(input_part*kernel_1))\n",
    "\n",
    "kernel_2 = np.array([\n",
    "    [[-0.1619,  0.1820, -0.1211],\n",
    "     [0.1733,  0.1670, -0.0993],\n",
    "     [0.0322, -0.1547,  0.1175]],\n",
    "\n",
    "    [[-0.1096, -0.1338,  0.1487],\n",
    "     [-0.0573,  0.1454,  0.1511],\n",
    "     [0.1339, -0.0923, -0.1172]],\n",
    "\n",
    "    [[-0.0601, -0.0949,  0.0579],\n",
    "     [-0.1379,  0.0424,  0.1474],\n",
    "     [0.1801, -0.1381, -0.1706]]\n",
    "])\n",
    "print(np.sum(input_part*kernel_2))\n",
    "# 结果和图上会有些许差异，主要是四舍五入所导致的误差\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03c889",
   "metadata": {},
   "source": [
    "## 图解stride\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/stride2d.svg\">\n",
    "<img src=\"./imgs/stride2d.svg\"\n",
    "    width=\"800\" /></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038c746",
   "metadata": {},
   "source": [
    "## 图解padding\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/padding2d.svg\">\n",
    "<img src=\"./imgs/padding2d.svg\"\n",
    "    width=\"800\" /></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc06b5",
   "metadata": {},
   "source": [
    "## 图解dilation\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/dilation2d.svg\">\n",
    "<img src=\"./imgs/dilation2d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e081de",
   "metadata": {},
   "source": [
    "## 图解groups\n",
    "<p align=\"center\">\n",
    "<a href=\"https://raw.githubusercontent.com/itmorn/AI.handbook/main/DL/module/Convolution/imgs/groups2d.svg\">\n",
    "<img src=\"./imgs/groups2d.svg\"\n",
    "    width=\"2000\" /></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04c841",
   "metadata": {},
   "source": [
    "# Conv3d\n",
    "对由多个输入平面组成的输入信号应用2D卷积。\n",
    "\n",
    "定义：  \n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "参数：  \n",
    "in_channels (int): Number of channels in the input image\n",
    "输入图像中的通道数\n",
    "\n",
    "out_channels (int): Number of channels produced by the convolution\n",
    "卷积产生的通道数\n",
    "\n",
    "kernel_size (int or tuple): Size of the convolving kernel\n",
    "卷积核的大小\n",
    "\n",
    "stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "卷积步幅。默认值:1\n",
    "\n",
    "padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: 0\n",
    "输入框四个边的添加的Padding尺寸。默认值:0\n",
    "\n",
    "padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
    "添加Padding的方式\n",
    "\n",
    "dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "卷积核元素之间的间距\n",
    "\n",
    "groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
    "分组卷积\n",
    "\n",
    "bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
    "添加一个可学习的偏置项\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78102bae",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "\n",
    "[《人工智能导论：模型与算法》](https://item.jd.com/12653461.html)\n",
    "\n",
    "[《统计学习方法》](https://item.jd.com/12522197.html)\n",
    "\n",
    "[《机器学习》](https://item.jd.com/12762673.html)\n",
    "\n",
    "[FIVE KEY ASSUMPTIONS OF LINEAR REGRESSION ALGORITHM](https://dataaspirant.com/assumptions-of-linear-regression-algorithm/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e00c79739f2fdf113306667eb0b8e68d4274855301e6df90bc305a954991b52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
